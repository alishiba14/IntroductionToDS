{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analytics\n",
    "In this lab, we will go through a quiz for enhancing the understading about NoSQL and Map/Reduce paradigm, and practice to:    \n",
    "    - Process data in [JSON](https://en.wikipedia.org/wiki/JSON) format     \n",
    "    - Design and write Map/Reduce programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoSQL Quiz\n",
    "1. Which of the following is true about NoSQL?    \n",
    "    - (a) An alternative to SQL databases only for managing textual data\n",
    "    - (b) A distributed version of relationl databases\n",
    "    - (c) A paradigm only for managing non-structured data\n",
    "    - (d) A new data paradigm to manage large datasets - <b>True</b>\n",
    "2. NoSQL is optimized for scaling up\n",
    "    - (a) number of data rows\n",
    "    - (b) number of data columns - <b>True</b>\n",
    "2. Does NoSQL databases prohibits the use of SQL?\n",
    "    - (a) Yes\n",
    "    - (b) No  - <b>True</b>\n",
    "3. Which of the following represents a column in NoSQL?\n",
    "    - (a) Document\n",
    "    - (b) Collection\n",
    "    - (c) Database\n",
    "    - (d) Field  - <b>True</b>\n",
    "5. Is Hadoop a NoSQL program?\n",
    "    - (a) Yes\n",
    "    - (b) No - <b>True</b>\n",
    "    - (c) Case by case\n",
    "    - (d) Depends on the provider and version \n",
    "\n",
    "## Map/Reduce Quiz\n",
    "1. Which of the following is true about Map/Reduce programing model\n",
    "    - (a) It is used to write programs that process data in stream\n",
    "    - (b) It is used to write programs that process data in parallel  - <b>True</b>\n",
    "    - (c) It only works within Hadoop framework\n",
    "    - (d) It only works with data of key-value pair types\n",
    "2. Which of the following describes the map function?\n",
    "    - (a) It maps a key-value pair to another key-value pair\n",
    "    - (b) It finds a value coressonping to a key\n",
    "    - (c) It manipulates the value of a key-value pair\n",
    "    - (d) It prosesses data to create a list of key-value pairs  - <b>True</b>\n",
    "3. Which of the following is true about output of the mappers?\n",
    "    - (a) Each mapper must generate the same number of key-value pairs as its input had.    \n",
    "    - (b) All mappers must generate the same number of key-value pairs    \n",
    "    - (c) Each mapper can generate zero or some key/value pair(s)  - <b>True</b>\n",
    "    - (d) Each mapper generates some key-value pair(s) of same type with its input\n",
    "4. Which of the following describes the reduce function?\n",
    "    - (a) It reduces values that have the same key\n",
    "    - (b) It reduces the number of values having the same key\n",
    "    - (c) It aggregates values having the same key into one of same type\n",
    "    - (d) It produces a summary of values having same key  - <b>True</b>\n",
    "5. Which of the following is true about input of the reducers?\n",
    "    - (a) Each reducers take as input a list of key-value pairs\n",
    "    - (b) Each reducer takes as input a key and a list of values  - <b>True</b>\n",
    "    - (c) Each ruducer taskes as input the output of a dedicated group of mapper\n",
    "    - (d) Diferent reducers may have inputs with same key(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data in JSON format\n",
    "\n",
    "We will use [jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html) package for working with JSON object\n",
    "### Example\n",
    "Given a set of Twitter messages (i.e., <i> tweets </i>) contained in file, each line is a tweet in [JSON format](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object), to extract text content of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages\n",
    "\n",
    "#jsonlite package: to i/o with data from/to JSON format\n",
    "#install.packages(\"jsonlite\", repos = \"https://cran.uni-muenster.de/\")\n",
    "library(jsonlite)\n",
    "\n",
    "#\n",
    "library(data.table)\n",
    "library(plyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to extract text content from tweet in JSON format\n",
    "get_text <- function(jsonTweet){\n",
    "    tweet <- fromJSON(jsonTweet)\n",
    "    return(tweet['text'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract tweets' text content from JSON file\n",
    "readTweets <- function(filename){\n",
    "    tweets <- c()\n",
    "    #open file\n",
    "    conn <- file(filename,open=\"r\")\n",
    "    #set of tweets\n",
    "    lines <-readLines(conn)\n",
    "    #process each tweet in turn\n",
    "    for (i in 1:length(lines)){\n",
    "       tweets <- c(tweets, get_text(lines[i]))\n",
    "    }\n",
    "    close(conn)\n",
    "    #close file\n",
    "    return (tweets)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'Seems like @united didn\\'t think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @JayseDavid: @United overbook #flight3411 and decided to force random passengers off the plane. Here\\'s how they did it: https://t.co/Qfe…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd><span style=white-space:pre-wrap>'Elderly passenger ripped off overbooked flight  https://t.co/iVVv9lXxe3'</span></dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd><span style=white-space:pre-wrap>'@united why overbook flights when you still get peoples money wether they show or not.  #nologic'</span></dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'Shock as man forcibly removed from overbooked United Airlines flight https://t.co/Jv9JyP1M0m'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @CNN @FoxNews @WHAS11 Man forcibly removed from plane somehow gets back on still bloody from being removed https…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @TelegraphWorld: Shock as man forcibly removed from overbooked flight https://t.co/9BgAFTvq8D https://t.co/77VkSqKnET'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'I will never fly @united again after seeing them drag a doctor off their plane. #dontflyunited that video is ridiculous.'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd><span style=white-space:pre-wrap>'RT @striders: #UnitedAirlines sent security to violently drag a doctor from flight because United is overbooked. @united @ACLU  https://t.c…'</span></dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @USAnonymous: to make way for United employees as the flight was overbooked. https://t.co/E5wVCHmrvf #2'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'</dd>\n",
       "\t<dt>$text</dt>\n",
       "\t\t<dd>'Seems like @united didn\\'t think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$text] 'Seems like @united didn\\textbackslash{}'t think the leggings controversy was enough. \\#bouttogetsued \\#howisthislegal?… https://t.co/L8Fd5WKKPu'\n",
       "\\item[\\$text] 'RT @JayseDavid: @United overbook \\#flight3411 and decided to force random passengers off the plane. Here\\textbackslash{}'s how they did it: https://t.co/Qfe…'\n",
       "\\item[\\$text] 'Elderly passenger ripped off overbooked flight  https://t.co/iVVv9lXxe3'\n",
       "\\item[\\$text] '@united why overbook flights when you still get peoples money wether they show or not.  \\#nologic'\n",
       "\\item[\\$text] 'Shock as man forcibly removed from overbooked United Airlines flight https://t.co/Jv9JyP1M0m'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @CNN @FoxNews @WHAS11 Man forcibly removed from plane somehow gets back on still bloody from being removed https…'\n",
       "\\item[\\$text] 'RT @TelegraphWorld: Shock as man forcibly removed from overbooked flight https://t.co/9BgAFTvq8D https://t.co/77VkSqKnET'\n",
       "\\item[\\$text] 'I will never fly @united again after seeing them drag a doctor off their plane. \\#dontflyunited that video is ridiculous.'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "\\item[\\$text] 'RT @striders: \\#UnitedAirlines sent security to violently drag a doctor from flight because United is overbooked. @united @ACLU  https://t.c…'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "\\item[\\$text] 'RT @USAnonymous: to make way for United employees as the flight was overbooked. https://t.co/E5wVCHmrvf \\#2'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "\\item[\\$text] 'RT @Tyler\\_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "\\item[\\$text] 'Seems like @united didn\\textbackslash{}'t think the leggings controversy was enough. \\#bouttogetsued \\#howisthislegal?… https://t.co/L8Fd5WKKPu'\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$text\n",
       ":   'Seems like @united didn\\'t think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu'\n",
       "$text\n",
       ":   'RT @JayseDavid: @United overbook #flight3411 and decided to force random passengers off the plane. Here\\'s how they did it: https://t.co/Qfe…'\n",
       "$text\n",
       ":   <span style=white-space:pre-wrap>'Elderly passenger ripped off overbooked flight  https://t.co/iVVv9lXxe3'</span>\n",
       "$text\n",
       ":   <span style=white-space:pre-wrap>'@united why overbook flights when you still get peoples money wether they show or not.  #nologic'</span>\n",
       "$text\n",
       ":   'Shock as man forcibly removed from overbooked United Airlines flight https://t.co/Jv9JyP1M0m'\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @CNN @FoxNews @WHAS11 Man forcibly removed from plane somehow gets back on still bloody from being removed https…'\n",
       "$text\n",
       ":   'RT @TelegraphWorld: Shock as man forcibly removed from overbooked flight https://t.co/9BgAFTvq8D https://t.co/77VkSqKnET'\n",
       "$text\n",
       ":   'I will never fly @united again after seeing them drag a doctor off their plane. #dontflyunited that video is ridiculous.'\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "$text\n",
       ":   <span style=white-space:pre-wrap>'RT @striders: #UnitedAirlines sent security to violently drag a doctor from flight because United is overbooked. @united @ACLU  https://t.c…'</span>\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "$text\n",
       ":   'RT @USAnonymous: to make way for United employees as the flight was overbooked. https://t.co/E5wVCHmrvf #2'\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "$text\n",
       ":   'RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…'\n",
       "$text\n",
       ":   'Seems like @united didn\\'t think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$text\n",
       "[1] \"Seems like @united didn't think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @JayseDavid: @United overbook #flight3411 and decided to force random passengers off the plane. Here's how they did it: https://t.co/Qfe…\"\n",
       "\n",
       "$text\n",
       "[1] \"Elderly passenger ripped off overbooked flight  https://t.co/iVVv9lXxe3\"\n",
       "\n",
       "$text\n",
       "[1] \"@united why overbook flights when you still get peoples money wether they show or not.  #nologic\"\n",
       "\n",
       "$text\n",
       "[1] \"Shock as man forcibly removed from overbooked United Airlines flight https://t.co/Jv9JyP1M0m\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @CNN @FoxNews @WHAS11 Man forcibly removed from plane somehow gets back on still bloody from being removed https…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @TelegraphWorld: Shock as man forcibly removed from overbooked flight https://t.co/9BgAFTvq8D https://t.co/77VkSqKnET\"\n",
       "\n",
       "$text\n",
       "[1] \"I will never fly @united again after seeing them drag a doctor off their plane. #dontflyunited that video is ridiculous.\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @striders: #UnitedAirlines sent security to violently drag a doctor from flight because United is overbooked. @united @ACLU  https://t.c…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @USAnonymous: to make way for United employees as the flight was overbooked. https://t.co/E5wVCHmrvf #2\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…\"\n",
       "\n",
       "$text\n",
       "[1] \"RT @Tyler_Bridges: @united @FoxNews @CNN not a good way to treat a Doctor trying to get to work because they overbooked https://t.co/sj9oHk…\"\n",
       "\n",
       "$text\n",
       "[1] \"Seems like @united didn't think the leggings controversy was enough. #bouttogetsued #howisthislegal?… https://t.co/L8Fd5WKKPu\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test the function\n",
    "#get list of tweets' text content\n",
    "tweets <- readTweets(\"data/ua.txt\")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Given a set of Twitter messages (i.e., tweets ) contained in file, each line is a tweet in JSON format, to count number of tweets per users in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 14 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>user</th><th scope=col>num_tweets</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Andyx002       </td><td>1</td></tr>\n",
       "\t<tr><td>Asimxo         </td><td>1</td></tr>\n",
       "\t<tr><td>boredinohio2   </td><td>1</td></tr>\n",
       "\t<tr><td>cgnetwork      </td><td>1</td></tr>\n",
       "\t<tr><td>IndexJoker     </td><td>1</td></tr>\n",
       "\t<tr><td>JaneReid5      </td><td>1</td></tr>\n",
       "\t<tr><td>Matthew_F944   </td><td>1</td></tr>\n",
       "\t<tr><td>melman101      </td><td>1</td></tr>\n",
       "\t<tr><td>MikePence2Cents</td><td>2</td></tr>\n",
       "\t<tr><td>MooreOfKaitlyn </td><td>1</td></tr>\n",
       "\t<tr><td>paultower      </td><td>1</td></tr>\n",
       "\t<tr><td>sanagnos       </td><td>1</td></tr>\n",
       "\t<tr><td>twintailsx     </td><td>2</td></tr>\n",
       "\t<tr><td>ZeimusCS       </td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 14 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       " user & num\\_tweets\\\\\n",
       " <fct> & <int>\\\\\n",
       "\\hline\n",
       "\t Andyx002        & 1\\\\\n",
       "\t Asimxo          & 1\\\\\n",
       "\t boredinohio2    & 1\\\\\n",
       "\t cgnetwork       & 1\\\\\n",
       "\t IndexJoker      & 1\\\\\n",
       "\t JaneReid5       & 1\\\\\n",
       "\t Matthew\\_F944    & 1\\\\\n",
       "\t melman101       & 1\\\\\n",
       "\t MikePence2Cents & 2\\\\\n",
       "\t MooreOfKaitlyn  & 1\\\\\n",
       "\t paultower       & 1\\\\\n",
       "\t sanagnos        & 1\\\\\n",
       "\t twintailsx      & 2\\\\\n",
       "\t ZeimusCS        & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 14 × 2\n",
       "\n",
       "| user &lt;fct&gt; | num_tweets &lt;int&gt; |\n",
       "|---|---|\n",
       "| Andyx002        | 1 |\n",
       "| Asimxo          | 1 |\n",
       "| boredinohio2    | 1 |\n",
       "| cgnetwork       | 1 |\n",
       "| IndexJoker      | 1 |\n",
       "| JaneReid5       | 1 |\n",
       "| Matthew_F944    | 1 |\n",
       "| melman101       | 1 |\n",
       "| MikePence2Cents | 2 |\n",
       "| MooreOfKaitlyn  | 1 |\n",
       "| paultower       | 1 |\n",
       "| sanagnos        | 1 |\n",
       "| twintailsx      | 2 |\n",
       "| ZeimusCS        | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "   user            num_tweets\n",
       "1  Andyx002        1         \n",
       "2  Asimxo          1         \n",
       "3  boredinohio2    1         \n",
       "4  cgnetwork       1         \n",
       "5  IndexJoker      1         \n",
       "6  JaneReid5       1         \n",
       "7  Matthew_F944    1         \n",
       "8  melman101       1         \n",
       "9  MikePence2Cents 2         \n",
       "10 MooreOfKaitlyn  1         \n",
       "11 paultower       1         \n",
       "12 sanagnos        1         \n",
       "13 twintailsx      2         \n",
       "14 ZeimusCS        1         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function to extract user's screen_name from tweet in JSON format\n",
    "get_user <- function(jsonTweet){\n",
    "    tweet <- fromJSON(jsonTweet)\n",
    "    user <-  as.list(tweet['user'])\n",
    "    f <- as.data.frame(do.call(rbind, as.list(user)))\n",
    "    f <- setDT(f, keep.rownames = TRUE)[]    \n",
    "    return(f$screen_name)\n",
    "}\n",
    "#function to extract users from JSON file\n",
    "readUsers <- function(filename){\n",
    "    users <- c()\n",
    "    #open file\n",
    "    conn <- file(filename,open=\"r\")\n",
    "    #set of tweets\n",
    "    lines <-readLines(conn)\n",
    "    #process each tweet in turn\n",
    "    for (i in 1:length(lines)){\n",
    "       users <- c(users, get_user(lines[i]))\n",
    "    }\n",
    "    close(conn)\n",
    "    #close file\n",
    "    \n",
    "    #count number of tweets per user\n",
    "    f <- as.data.frame(do.call(rbind, as.list(users)))\n",
    "    f <- setDT(f, keep.rownames = TRUE)\n",
    "    names(f) <- c(\"tweet_index\", \"user\")\n",
    "    f <- count(f, 'user')\n",
    "    names(f) <- c(\"user\", \"num_tweets\")\n",
    "    return (f)\n",
    "}\n",
    "#test the function\n",
    "users <- readUsers(\"data/ua.txt\")\n",
    "users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Map/Reduce programs\n",
    "\n",
    "We will use the [DSL](https://cran.r-project.org/web/packages/DSL/index.html) package for (simulating) the Map/Reduce programming environment\n",
    "\n",
    "### Example\n",
    "Given a large set of tweets containted in multiple files in JSON format, to count number of tweets containing each hashtag in the set\n",
    "\n",
    "Main steps:\n",
    "1. Create a list of (directory, filename) pairs\n",
    "2. Use a mapper to turn the (directory, filename) pairs files into (key = filename, value = tweet's text content) pairs \n",
    "3. Use another mapper to turn (key = filename, value = tweet's text content) pairs to (key = hashtag, value = count) pairs\n",
    "4. Use a reducer to sum up the counts of the same hashtags from (key = hashtag, value = count) pairs\n",
    "\n",
    "<div style='float: center'>\n",
    "  <img style='display' src=\"image/hashtagCount.jpg\"></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages\n",
    "\n",
    "#DSL packaage \n",
    "#install.packages(\"DSL\", repos = \"https://cran.uni-muenster.de/\")\n",
    "\n",
    "#import libraries\n",
    "library(DSL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into tweets: this function turns a (key = directory, value = filename) pair\n",
    "## to (key = filename, value = tweet's text content) pairs\n",
    "split_tweets <- function( keypair ){\n",
    "    #re-use the readTweets function written above\n",
    "    values <- readTweets(keypair$value)\n",
    "    #produce \n",
    "    mapply( function(key, value) list( key = key, value = value), rep(keypair$key, length(values)), values,\n",
    "            SIMPLIFY = FALSE, USE.NAMES = FALSE )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extract hashtags in a tweet\n",
    "# if the input tweet has no hashtag, \"#NO_HASHTAG\" is returned\n",
    "get_hashtags <- function(s){\n",
    "    hashtags <- grep(\"^[#]\", scan(textConnection(s), \"\"), value=TRUE)\n",
    "    hashtags <- unique(hashtags)\n",
    "    if(length(hashtags) == 0){\n",
    "        hashtags <- c(\"#NO_HASHTAG\")\n",
    "    }\n",
    "    return (hashtags)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function turns a (key = filename, value = tweet's text content) pair\n",
    "## to (key = hashtag, value = count) pairs\n",
    "split_hashtags <- function( keypair ){\n",
    "    keys <- get_hashtags(keypair$value)\n",
    "    mapply( function(key, value) list( key = key, value = value), keys, rep(1L, length(keys)),\n",
    "            SIMPLIFY = FALSE, USE.NAMES = FALSE )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$london_attack.txt</dt>\n",
       "\t\t<dd>'./data/london_attack.txt'</dd>\n",
       "\t<dt>$ua.txt</dt>\n",
       "\t\t<dd>'./data/ua.txt'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$london\\_attack.txt] './data/london\\_attack.txt'\n",
       "\\item[\\$ua.txt] './data/ua.txt'\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$london_attack.txt\n",
       ":   './data/london_attack.txt'\n",
       "$ua.txt\n",
       ":   './data/ua.txt'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$london_attack.txt\n",
       "[1] \"./data/london_attack.txt\"\n",
       "\n",
       "$ua.txt\n",
       "[1] \"./data/ua.txt\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"********************\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.table: 32 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>hashtags</th><th scope=col>num_tweets</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>#PARLIAMENT           </td><td> 1</td></tr>\n",
       "\t<tr><td>#UK                   </td><td> 7</td></tr>\n",
       "\t<tr><td>#London.After         </td><td> 1</td></tr>\n",
       "\t<tr><td>#2                    </td><td> 1</td></tr>\n",
       "\t<tr><td>#Banislam             </td><td> 1</td></tr>\n",
       "\t<tr><td>#Brus…                </td><td> 1</td></tr>\n",
       "\t<tr><td>#Westminster          </td><td> 8</td></tr>\n",
       "\t<tr><td>#LONDON               </td><td> 1</td></tr>\n",
       "\t<tr><td>#BritainAttack        </td><td> 1</td></tr>\n",
       "\t<tr><td>#Bridge!!!!           </td><td> 1</td></tr>\n",
       "\t<tr><td>#NO_HASHTAG           </td><td>81</td></tr>\n",
       "\t<tr><td>#NothingToDoWithIslam.</td><td> 1</td></tr>\n",
       "\t<tr><td>#westminster          </td><td> 2</td></tr>\n",
       "\t<tr><td>#LondonAttack         </td><td> 2</td></tr>\n",
       "\t<tr><td>#Parliament           </td><td> 2</td></tr>\n",
       "\t<tr><td>#flight3411           </td><td> 1</td></tr>\n",
       "\t<tr><td>#terror               </td><td> 2</td></tr>\n",
       "\t<tr><td>#London               </td><td> 8</td></tr>\n",
       "\t<tr><td>#UnitedAirlines       </td><td> 1</td></tr>\n",
       "\t<tr><td>#FakeNewsCNN          </td><td> 1</td></tr>\n",
       "\t<tr><td>#terrorism♦️           </td><td> 1</td></tr>\n",
       "\t<tr><td>#London,              </td><td> 2</td></tr>\n",
       "\t<tr><td>#London.              </td><td> 1</td></tr>\n",
       "\t<tr><td>#London:              </td><td> 3</td></tr>\n",
       "\t<tr><td>##Possibly            </td><td> 1</td></tr>\n",
       "\t<tr><td>#bouttogetsued        </td><td> 2</td></tr>\n",
       "\t<tr><td>#BritishParliament    </td><td> 1</td></tr>\n",
       "\t<tr><td>#dontflyunited        </td><td> 1</td></tr>\n",
       "\t<tr><td>#England              </td><td> 1</td></tr>\n",
       "\t<tr><td>#nologic              </td><td> 1</td></tr>\n",
       "\t<tr><td>#howisthislegal?…     </td><td> 2</td></tr>\n",
       "\t<tr><td>#WestminsterAttack    </td><td> 3</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 32 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       " hashtags & num\\_tweets\\\\\n",
       " <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t \\#PARLIAMENT            &  1\\\\\n",
       "\t \\#UK                    &  7\\\\\n",
       "\t \\#London.After          &  1\\\\\n",
       "\t \\#2                     &  1\\\\\n",
       "\t \\#Banislam              &  1\\\\\n",
       "\t \\#Brus…                 &  1\\\\\n",
       "\t \\#Westminster           &  8\\\\\n",
       "\t \\#LONDON                &  1\\\\\n",
       "\t \\#BritainAttack         &  1\\\\\n",
       "\t \\#Bridge!!!!            &  1\\\\\n",
       "\t \\#NO\\_HASHTAG            & 81\\\\\n",
       "\t \\#NothingToDoWithIslam. &  1\\\\\n",
       "\t \\#westminster           &  2\\\\\n",
       "\t \\#LondonAttack          &  2\\\\\n",
       "\t \\#Parliament            &  2\\\\\n",
       "\t \\#flight3411            &  1\\\\\n",
       "\t \\#terror                &  2\\\\\n",
       "\t \\#London                &  8\\\\\n",
       "\t \\#UnitedAirlines        &  1\\\\\n",
       "\t \\#FakeNewsCNN           &  1\\\\\n",
       "\t \\#terrorism♦️            &  1\\\\\n",
       "\t \\#London,               &  2\\\\\n",
       "\t \\#London.               &  1\\\\\n",
       "\t \\#London:               &  3\\\\\n",
       "\t \\#\\#Possibly             &  1\\\\\n",
       "\t \\#bouttogetsued         &  2\\\\\n",
       "\t \\#BritishParliament     &  1\\\\\n",
       "\t \\#dontflyunited         &  1\\\\\n",
       "\t \\#England               &  1\\\\\n",
       "\t \\#nologic               &  1\\\\\n",
       "\t \\#howisthislegal?…      &  2\\\\\n",
       "\t \\#WestminsterAttack     &  3\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 32 × 2\n",
       "\n",
       "| hashtags &lt;chr&gt; | num_tweets &lt;int&gt; |\n",
       "|---|---|\n",
       "| #PARLIAMENT            |  1 |\n",
       "| #UK                    |  7 |\n",
       "| #London.After          |  1 |\n",
       "| #2                     |  1 |\n",
       "| #Banislam              |  1 |\n",
       "| #Brus…                 |  1 |\n",
       "| #Westminster           |  8 |\n",
       "| #LONDON                |  1 |\n",
       "| #BritainAttack         |  1 |\n",
       "| #Bridge!!!!            |  1 |\n",
       "| #NO_HASHTAG            | 81 |\n",
       "| #NothingToDoWithIslam. |  1 |\n",
       "| #westminster           |  2 |\n",
       "| #LondonAttack          |  2 |\n",
       "| #Parliament            |  2 |\n",
       "| #flight3411            |  1 |\n",
       "| #terror                |  2 |\n",
       "| #London                |  8 |\n",
       "| #UnitedAirlines        |  1 |\n",
       "| #FakeNewsCNN           |  1 |\n",
       "| #terrorism♦️            |  1 |\n",
       "| #London,               |  2 |\n",
       "| #London.               |  1 |\n",
       "| #London:               |  3 |\n",
       "| ##Possibly             |  1 |\n",
       "| #bouttogetsued         |  2 |\n",
       "| #BritishParliament     |  1 |\n",
       "| #dontflyunited         |  1 |\n",
       "| #England               |  1 |\n",
       "| #nologic               |  1 |\n",
       "| #howisthislegal?…      |  2 |\n",
       "| #WestminsterAttack     |  3 |\n",
       "\n"
      ],
      "text/plain": [
       "   hashtags               num_tweets\n",
       "1  #PARLIAMENT             1        \n",
       "2  #UK                     7        \n",
       "3  #London.After           1        \n",
       "4  #2                      1        \n",
       "5  #Banislam               1        \n",
       "6  #Brus…                  1        \n",
       "7  #Westminster            8        \n",
       "8  #LONDON                 1        \n",
       "9  #BritainAttack          1        \n",
       "10 #Bridge!!!!             1        \n",
       "11 #NO_HASHTAG            81        \n",
       "12 #NothingToDoWithIslam.  1        \n",
       "13 #westminster            2        \n",
       "14 #LondonAttack           2        \n",
       "15 #Parliament             2        \n",
       "16 #flight3411             1        \n",
       "17 #terror                 2        \n",
       "18 #London                 8        \n",
       "19 #UnitedAirlines         1        \n",
       "20 #FakeNewsCNN            1        \n",
       "21 #terrorism♦️             1        \n",
       "22 #London,                2        \n",
       "23 #London.                1        \n",
       "24 #London:                3        \n",
       "25 ##Possibly              1        \n",
       "26 #bouttogetsued          2        \n",
       "27 #BritishParliament      1        \n",
       "28 #dontflyunited          1        \n",
       "29 #England                1        \n",
       "30 #nologic                1        \n",
       "31 #howisthislegal?…       2        \n",
       "32 #WestminsterAttack      3        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#directory containing the input files\n",
    "working_dir <- \"./data\"\n",
    "## SIMULATING cluster evironment\n",
    "ds <- DStorage(\"LFS\", tempdir(), chunksize = 1L)\n",
    "#step 1. Create a list of (directory, filename) pairs\n",
    "dl <- as.DList(working_dir, DStorage = ds)\n",
    "#uncomment these 2 following lines to see the intermediate results\n",
    "as.list(dl)\n",
    "print(\"********************\")\n",
    "\n",
    "\n",
    "#step 2. Use a mapper to turn the (directory, filename) pairs files into (key = filename, value = tweet's text content) pairs  \n",
    "dl <- DMap(dl, split_tweets)\n",
    "#as.list(dl)\n",
    "#print(\"********************\")\n",
    "\n",
    "#step 3. Use another mapper to turn (key = filename, value = tweet's text content) pairs to (key = hashtag, value = count) pairs\n",
    "dl <- DMap(dl, split_hashtags)\n",
    "#as.list(dl)\n",
    "#print(\"********************\")\n",
    "\n",
    "#step 4. Use a reducer to sum up the counts of the same hashtags from (key = hashtag, value = count) pairs\n",
    "dl <- DReduce(dl, sum )\n",
    "\n",
    "#convert the counts to a data table\n",
    "f <- as.data.frame(do.call(rbind, as.list(dl)))\n",
    "f <- setDT(f, keep.rownames = TRUE)[]\n",
    "names(f) <- c(\"hashtags\", \"num_tweets\")\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "<b>Design</b> <i>Map/Reduce algorithm</i> and write program (<b>optional!!!</b>) for the following tasks\n",
    "\n",
    "1. Given a set of Twitter messages (i.e., tweets ) contained in multiple files, each line is a tweet in JSON format, to count number of tweets per day in the set\n",
    "2. Given a large directed graph contained in multiple files, each file is a set of edges, each edge on a line, to count number of in-going and out-going edges per node in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suggestion\n",
    "\n",
    "Main steps in task 1:\n",
    "1. Create a list of (directory, filename) pairs\n",
    "2. Use a mapper to turn the (directory, filename) pairs files into (key = filename, value = tweet's published date) pairs: This mapper reads the input file (as the key of the input pair), line by line; for each line, it extracts the published date of the tweet contained in the line, and outputs a (key = filename, value = tweet's published date).\n",
    "3. Use a reducer to sum up the counts of the same publication date from (key = publication date, value = 1) pairs\n",
    "\n",
    "Main steps in task 2:\n",
    "1. Create a list of (directory, filename) pairs\n",
    "2. Use a mapper to turn the (directory, filename) pairs files into (key = node_id, value = {1, \"out\"}) and (key = node_id, value = {1, \"in\"})  pairs: This mapper reads the input file (as the key of the input pair), line by line; for each line, which is an edge (u, v), it outputs a (key = u, value = {1, \"out\"}) pair and a (key = v, value = {1, \"in\"}). \n",
    "4. Use a reducer to sum up the \"out\" and \"in\" counts of the  (key = u, value = {1, \"out\"/\"in\"}) pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[jsonlite package](https://cran.r-project.org/web/packages/jsonlite/index.html)\n",
    "\n",
    "[DSL package](https://cran.r-project.org/web/packages/DSL/index.html)\n",
    "\n",
    "[Map/Reduce examples](https://highlyscalable.wordpress.com/2012/02/01/mapreduce-patterns/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
