{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> keywords:<font color='black'> \n",
    "    \n",
    "**Tasks:** regression, classification, clustering  \n",
    "**Algorithms:** linear regression, logistic regression, random forest, SVM, k-means  \n",
    "**Evaluation Measures:** RMSE, precision, recall, F1-score  \n",
    "**Concepts:** training and testing, overfitting and underfitting, cross-validation  \n",
    "**R packages used:** tidyverse, rpart, randomForest, e1071, caret, mice, mltest  \n",
    "  \n",
    "  \n",
    "    \n",
    "The goal of this lab session is to get familar with various machine learning based tasks in R. Many packages in R have similar interface that uses a formula and other parameters.\n",
    "\n",
    "**formula:** is a the to express the form of a model. For example, suppose you have a response variable y and independent variables x1, x2 and x3. To express that y depends linearly on x1, x2 and x3 you would use the formula `y ~ x1 + x2 + x3.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is the machine learning task of inferring a function from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is the processes to estimate the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quizz:** Which of the following tasks are suitable to solve as a regression problem?\n",
    "\n",
    "    ☐ Predicting the author of a book from a set of fiction authors.\n",
    "    ☑ Predicting housing prices.\n",
    "    ☐ Recognizing digits in an image of a handwritten text.\n",
    "    ☐ Flagging whether an email is a spam or not.\n",
    "    ☑ Predicting monthly sales of a store.\n",
    "    ☐ Predicting if subscribers will churn at the end of the month.\n",
    "    ☐ Personality prediction based on Twitter stream.\n",
    "    ☑ Predicting the number of edits a Wikipedia editor will make next month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "We will use the `lm()` function in the `stats` package which is part of base R. No external package needed.\n",
    "\n",
    "    lm_model <- lm(y ∼ x1 + x2, data=mydata)\n",
    "    summary(lm_model)\n",
    "\n",
    "The vector of coefficients for the model is contained in `lm_model$coefficients.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will start with building a simple model using the `cars` dataset that comes with R. The dataset contains the speed of cars and the distances taken to stop. In this example, we will build a linear regression model with only a **single feature**, i.e. to compute `dist` from `speed.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some first rows of the dataset\n",
    "head(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 2.2.1     ✔ purrr   0.2.4\n",
      "✔ tibble  1.4.1     ✔ dplyr   0.7.4\n",
      "✔ tidyr   0.7.2     ✔ stringr 1.2.0\n",
      "✔ readr   1.1.1     ✔ forcats 0.2.0\n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "`geom_smooth()` using method = 'loess'\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAADwCAIAAABXFyDtAAAACXBIWXMAABJ0AAASdAHeZh94\nAAAgAElEQVR4nO2dd2AU1fr3z8xsb9lUUkkjlR6K9CgGBekICgiCVK/3taCglCugcvX+FFH0\niiCKKF0hEC9IkRZKABOC1CQk2QDpIXU323dn3j9Gx3U32ewmm501eT5/zZw5M/Ps7Nnvnjnn\nOc+DURSFAAAAAM8DZ9sAAAAAoGlAoAEAADwUEGgAAAAPBQQaAADAQwGBBgAA8FBAoAEAADwU\nEGgAAAAPBQQaAADAQ+GwbYBr0Gq1Op3ODTcSCAR8Pl+tVptMJjfcrkm4XC6Hw9FqtWwZgBDy\n8vIym82NjY0s2iCRSDQaDUmSbBnA5/MFAoFGozEajWzZwOFwuFwuu41BJpORJMluYxCLxVqt\nlsXGwOPxhEKhVqs1GAzOnuvt7d3coQ4i0BRFmc1m99wIx3GSJN1zuyYhCAIhxKIBCCHWHwJC\nCMMws9nM4m/SExoDjuP0c2DLANoGxHaDRAix+0XQjcHlQgRDHAAAAB5Ku/eg7969e+DAgcLC\nwqqqqlGjRr388svMoZMnT6anp9+7d0+v1wcHB48dO3bUqFHM0aysrB07dpSUlHh5eaWkpMyY\nMQPDsPa2FgAAwHNod4HW6XRBQUFDhgzZvXu31aHTp09379594sSJIpEoIyPj888/N5lMY8aM\nQQjl5eWtW7duzJgxr7/+emFh4aZNm0iSnDVrVntbCwBAZ8ZsNu/bt+/KlSs4jg8ZMuTpp5+m\nB3DYot0FulevXr169UIIpaamWh16//33me3ExMSioqKLFy/SAp2amhoSErJ48WKEUHh4eHl5\neVpa2rRp0/h8fnsbDABA58RgMEyePPnXX3+ld3fu3Llv3769e/dyOKzN1XnQGLTBYPDy8qK3\nc3JykpKSmENJSUk6nU6hULBkGgAAHZ+NGzcy6kyTnp7+1VdfsWUP8hwvjpMnTxYUFCxatAgh\nRFFUfX29pesJvV1bW8uUNDY2vvnmm8zumDFjRo8e7QY7aQ8KsVjMYhxtDMNwHGf+zNiCIAh2\nbSAIQiqVsmgA/fIrFouFQiFbNnhCY/AEGwiCkEgkbbzIyZMnmyxcsWJFi+fSjUEoFDr7lm/f\nDckjBPr8+fObN29esmRJTEyMg6cYjUbL/7o+ffpwudz2sa4JWHzlYWB3aAwhhGGYO595k7Bu\nAEKIIAj6b5tFoDEgVzwEvV7fZKHjH60VjcG+Wx77QnP06NFvvvlm6dKlgwYNokswDJPL5XV1\ndUwdetvHx4cpkcvlp0+fZnZJkqypqXGDtSKRSCgUKpVKFtcm8Hg8Ho/H7roAX19fk8nU0NDA\nog1eXl4qlYpFP2ihUCgSiVQqVSvWJrgKLpfL5/PZbQw+Pj4kSdbX17Nog1Qq1Wg0bfRB7tmz\n5+3bt20LHdEWgUAgFosbGxubVHn7+Pr6NneIZYHeu3dvamrq22+/3bt3b8vyhISE7Ozs+fPn\n07vZ2dkCgSAqKoqpgGGYTCZjdjUajUajcYPB9MgGRVEsDnFQf8CWAZaWsG4Au1+E59jAlgFW\nlrBrQBttWLFixfHjxy27hgEBAUuXLnXkskwd1z6Hdn8zMhgMCoVCoVAYDIbGxkaFQlFUVEQf\n2rp16759+1544QWpVErXKS4upg9NmTKltLR0y5Yt9+/fP3PmzMGDBydMmAAuHAAAtB/BwcFH\njx4dP368n59fQEDAlClTjh075ufnx6JJWHv/7ykUitdee82yBMfxQ4cOIYSee+45lUpleSgw\nMJCZM83MzNy5c2dxcTG9UGXmzJl2Fqq4rQctEolEIlFDQ0MnH+Lw8/MzmUzsvtXK5XKlUsnu\nEIdYLFYqlewOcQgEAqvfkZvx9fUlSdKy4+l+ZDKZWq1mcam3QCCQSCSNjY2tCApk5z+g3Yc4\noqKifvrppyYP7dq1y86JAwYMGDBgQPsYBQAA8DfAg/ygAQAAAEtAoAEAADwUEGgAAAAPBQQa\nAADAQwGBBgAA8FBAoAEAANpKQUFBe1wWBBoAAKBNMIvvXA4INAAAQOtpP3VGINAAAACtpl3V\nGYFAAwAAtI72VmcEAg0AANAK3KDOCAQaAADAWdyjzggEGgAAwCncps4IBBoAAMBx3KnOCAQa\nAADAQdyszggEGgAAwBHcr84IBBoAAKBFWFFnxHrSWFeB47hAIHDDjTgcDkKIx+M5m1zdtTYQ\nBOGez2sHDMPYtQHDMD6fz2KuUi6XixDi8Xg4zlpHhyAIaAwIIYIg+Hx+O+U/KygooL9r+wag\nP5qEU9hvwB1EoDEMc49i0j9FFn+Q9N3d9nntwLoNtAEsCjSdJBPHcRafAzQGBoIg7KQtbTX5\n+fmO/N7pW7fiOdj/U+kgAm02m92WNJbD4eh0OtaTxqrVarYMQAgJhUKSJNm1gcvlajQadpPG\ncrlcnU7HetJYdr8IgUDAemMgCEKj0bg8aazjIxt039lgMLQiaaxYLG7uEIxBAwAANAFb486W\ngEADAABY4wnqjDrMEAcAAIBLKC8vz8zMxDAsPDxcKpU6cspDJT/7nn+fPq43BgQaAADgdz79\n9NOMjAyTyYQQ4nK5Tz311IgRI+yfkl3ks+tChEZPJPVAI7q72B4Y4gAAAEAIobS0tHPnztHq\njBAyGo1paWl5eXnN1dcbie3pUVtPRWv0BELoP7tQyUMXKyr0oAEAAFBRUdHp06dty0+dOhUX\nF9dE/SrJtjNR1So+UxLki0hX+3yCQAMA0NmhpwSrqqpsD5WXl1uVkBR28mbgT1khZvJPt+tB\nsbXrX/YxGUjnvezsAQINAECnhnHYaHLRk1VhlVKw/WxUUdWfnssSgWn28KJ+3dQCnk+jqx3i\nQaABAOi8WLrT+fr62vaXAwIC6A2KQhdy/Q/82lVv/HOgOT5EOWeEQi42IuT0Im9HAIEGAKAz\nYuvpnJycvHfvXttChJBSy911IeLGfTlTziHI8f3KUnqW465fXv4nINAAAHQ6mlyHMmDAgJKS\nkgsXLjAljz/+eO/eva/k+/5wqavG8Kdahvpo5j2mCPLWtredINAAAHQu7KwSnDx58qBBg4qK\nijAMi4qKEslCN/8Sfv2+N1MBx9DjPcvH9yvlEu6I0gUCDQBAZ8GRBdxBQUFBQUEIoewinz2/\nhDfq/hRJX6l+zoiimCCV1SmFhYWXL1++cuVKWFjYCy+8MGDAAFcZDAINAB0NiqKOHDmSmZnJ\n4XCGDx/+6KOPsm2RR+B4eI16DW/3hfCbD/4cccYwlJxYNXlAMY9jHT0xIyPjwIEDCKHLly9f\nvnz5xx9//Pzzz6dPn+4Sm0GgAaBDYTQap0+ffu7cOXr3s88+e/bZZ//73/+yaxXrOKjOFIUy\n7vofuBKmNfwZ1tlXapg9vCguWGlbX6VS/fTTT1aFy5cvHz16tFwut63vLCDQANCh2LhxI6PO\nNPv27Rs2bJir+nR/OxzvOD9U8ndfjMgtlTElGIaGxT18+pFiPrfpSNP379+3DQ2vVquzs7NH\njhzZOoMtAYEGgA6FbYcOIZSWltY5BdpBdTaZsV9uBv18Lchk/tPHOUCmmzXiXkyg9YizJc0l\n9HFVoh82BZqiqP379586daq6ulosFvfq1ev555/39/enj2ZlZe3YsaOkpMTLyyslJWXGjBnt\nkc8GADoYTWY2aWxsdL8l7OJ4x1lRKdl5IaK8TsiUYBj1eI/KCf1LuUQL+XrCw8M5HA4TX4lG\nIBD07dvXWYObhE2BTk1N3bNnz0svvdS9e/fq6urNmzf/+9///vTTTxFCeXl569atGzNmzOuv\nv15YWLhp0yaSJGfNmsWitUBHpba29rPPPsvMzOTz+SNGjHjxxRdZz8HaFrp37/7gwQOrwh49\nerBiDFs4qM4aA+fw1eCzdwIo6s/OX4iP5rlh9yIDHMrgJZPJxo4dm5aWZlm4bt06Hx8fpwxu\nDjYF+s6dO4mJiSkpKQihoKCgsWPHbt682Wg0crnc1NTUkJCQxYsXI4TCw8PLy8vT0tKmTZvG\n5/NbuioAOEF1dfVjjz1WUVFB754/f/7nn38+fPgwj8dj17BWs2rVqrNnz2q1f66h8PPzW7Jk\nCYsmuRPHJwMv5/sdzAxTaf/UQB6HHJtUmtKzEsecGKAYMWJEQEBARkZGXV1deHj4vHnzWgwh\n7ThsxoPu2bNnQUFBbm4uQqiuru7ChQtJSUl07sWcnJykpCSmZlJSkk6nUygUrNkKdFDeeecd\nRp1prl279tVXX7FlT9uJi4s7ePDgoEGDeDyeQCBISUlJS0tjAkp0bBxU59Ja4YYjCd+fi7RU\n58TQhrefvvVErwqn1JkmPj5+8eLFN2/e3Lt3rwvVGbHbg540aZLJZFqxYgVCyGw2JyUlLV++\nHCFEUVR9fb2395+rd+jt2tpapkSpVM6ePZvZnT59+jPPPOMGm+kE7FKp1FWTAK0AwzAMw+h/\nMhYhCMLyO3I/OI7L5fI2fhGXL19usvDtt99u8Vx6UkQikbDeGKy+iJSUlJSUFKPRiOM4QRDN\nnetCG1hvDHRYfZFIZL+aRk8czvI7/puvyfznmIZcbHpmaOXQ+HqECIRauEJz0I1BJBIJhcIW\nK1tiPy09mwJ98eLF1NTUxYsXJyQkVFdXb9++/cMPP3TkhwEAroJFbW1vcByn+xMdG/oVvEUP\nApJC5+94/5gRYNlrxjFqVO/ayYOqhLwWJgPZgk2B/uabb0aOHDl69GiEUHh4uEQiWbZsWV5e\nXnx8vFwur6urY2rS25bj7jKZzHJgXqPRWNZvP0QikUgkUqlUts6PboPH4/F4PHbn5f38/Mxm\nc319PYs2yOVypVJpvwPSIoMHD75//75V4SOPPOJIcxIKhWKxuLGx0WBwdRhgh+FyuQKBQKX6\niyvYtWvX1q5dm5WVxeFwhg0b9s4773Tr1q39bPD19SVJ0j0/QEssBzQEAoHBYGiuMSgqJT9c\n6nq/WmxZGBXQOGPo/VBfDWVCGlOT5zkB/Uar0Wh0zkfs9/Pza+4QmwKt1+st/+Hp/0Cz2YwQ\nSkhIyM7Onj9/Pn0oOztbIBBERUWxYifQgVm9evWZM2cqKyuZkt69e9Oz039T7t69O2nSJI1G\ngxAyGAwnTpy4du1aeno648DaMXBwuLlezTuUFfprga/lm5KYb3qqb9mj3StdFSk0MjKynTx/\n2BTowYMHHzt2LCIiIj4+vqamZtu2bV26dKH/6qdMmfLWW29t2bJl9OjRCoXi4MGDkyZNAhcO\nwOX4+/unp6dv3Ljx119/5fP5jz766Isvvvj3deFACP373/+m1Znh4cOHGzZs+OCDD9gyybU4\nKM16I37yZtDx64FGi7UnOEYlJ1aN71cq5DW9MtApIiMj234R+7Ap0AsXLpTJZHv37q2trRWL\nxYmJic8//zytwnFxcatWrdq5c+fx48e9vLwmT548c+ZMFk0FOjC+vr7vvvsu21a4jFu3bjlY\n+LfDQWkmKexint+R7JAGzV8m0hNClM8MfhAod0EQZzdIMw2bAs3n859//vnnn3++yaMDBgxw\nYdQ+AOgkiMVi20KJROJ+S1yI48sCb9yXH8wMraj/iytFFy/d048U9+za1ikTt+kyA8TiAIAO\nxfjx43NycqwKJ0yYwIoxbceZFdvi1Csh+RVSy0IR3zS2b1lyYhWBt95dx/26zAACDQAdildf\nfTUjI8Myb9PUqVP/jpGSHJfmsjrh4eyu14pkloUcgkpOrBrTp0zMb72LBovSTAMCDQAdCh6P\nl5qaevjw4StXrnC53BEjRjz22GNsG+UEjusyQqhGxf9fdkhmgS9p0T/GMNQvqnZS/xJfqd7Z\nu6vV6pMnTx45cgTH8WHDhr3yyiteXl7OXsSFgEADQEcDw7Dx48ePHz+ebUOcwylprlPzjl4L\nupTvb7kmECEUG6Sc8khJuJ9DoY6s0Gg0s2fPLikpoXezs7MPHz586tQpFkfwQaABAEAIIY1G\nc/r06dLS0oiIiJEjR7ozloBT0lyv5h6/EXwx19/4V2kO99dM6FeSGNrQCgPooYzly5cz6kyj\nUCg++eQTFpc3g0ADAIB+++23uXPnlpaW0rsxMTG7du1q7xFYp3QZIVSv5v5yM+hcjr9lWH2E\nUKBcN2VQVa+whxTl9LJSy8+YkZFhW6HJQrcBAg0AnR2tVrtgwQJGnRFC+fn5ixYtOn78eDtF\n83BWmmtUvBM3gjLy/EzkX+zxlerHJZUN7FYjEvINBuR4YJUm/3ua/LDs5gkBgQaAzs7Fixdt\nA5L89ttvd+7ccW2kf2d1GSFUpRQc/y3wcr4vSVlJs2F077LBsdXO+s/ZeS0YMWLE7du3rQqT\nk5Odur5rAYEGgM5OTU1Nk+UPHz501S1aIc33H4pP3Aj67Z43+VcFxoyVYtWPr02O9/N1zr+i\nxRGbN99888SJE4WFhUxJYmLiq6++6tRdXItDAv3oo49++umnffr0sSo/ffr0u+++e/bsWdfb\nBQCAu2guDFmLMfBMJtPevXtv3LiB4/jAgQMnT55sNSDQCl2mKJRT6nXiRmBemczqEGYo5dbs\n4jQcJynj6VOPOBgC3vGRdIlEcurUqU2bNl2+fBnH8aFDhy5evJjd/GcOCXR6enqTgSWrqqrS\n09NdbRIAAG6lf//+I0eOPH36tGXhjBkzwsLC7Jyl1+snTJiQnZ1N737zzTc//PDD7t27cRxv\nhS4jhIxmLLPQ9/StwNJa65j3uOEBt3oHoTyJqN+DHFnlwWmSVkxyisXiZcuWOXtW+9GmIY76\n+vq/dXpNAAAQQhiGffnllytXrkxNTaUoiiCIOXPmrFmzxv5Z69evZ9SZ5tSpU7t27Ro2bJiz\nBqi0nPO5Ael3ApRaa9++CH+1oXhr/b1D6K8eGvaVh/UVgK7CnkDfuHHjxo0b9PYvv/xi5SFY\nW1v7+eefJyQktKN1AAC4BR8fn82bN69fv76kpCQ8PNyRvE3Hjx9ntocPH05v3L592ymBflAt\nupAbcKXA12Cy9qCI7tL4RO/yXl3rz541/q/I2n/OdsSVpsNIM409gU5NTX3nnXfo7ffff9+2\nglAo3Lt3b7vYBQCA25FIJPHx8Q5W1uv1jC4zOJhcxmTGrt3zOXs7QFFlvUiPS1D9o2se71EZ\n4vN7VOsRI0YoFApL/4r+/fvbhrqMioqi0310JOwJ9MyZM/v3748QGj9+/Pvvv9+zZ0/mEIZh\nUqm0T58+Mpn1QD4A/L0oKSn56KOPsrKyeDxecnLykiVL2A2/4PnQQ8zjxo27du2a1aGuXbva\nP7dezbuY559+x1+lsx7NEPLMg2Kqn+hVIRf/ReVxHJ83b15ubq5CocAwLCYmxnL2ku4yy2Qy\ntbo1y7s9HHsCHRsbGxsbixBas2bNjBkzIiIi3GQUALiLsrKyxx9/nEkYf+vWrZMnT/7yyy/O\n5mbu8NjO+40dOzYnJ8cyBZ+Xl1dKSkqTp5vM2M0H8vO5/nllXqSN43Kor+bRxKqB3Wq4RLNL\nAePj46169x1sNKNJHJokXLt2bTubAQDssHbtWkadafLy8jZt2vTGG2+wZZJHYccfw9vb+5VX\nXjl69KhCoSAIolu3bk899ZRtuoCKesGlu36X7vrZdpkxjOoR1jCye2V8iNIpqzqDNNNgjqSd\nV6lUSqUyJCSE3i0tLf3ss89qa2tnz549YsSIdrbQIfR6fRuzOzsIl8vlcDhuu12TEASB4ziL\nacURQkKhkCRJvd7pcI4uhM/nGwwGRxqwHWJjY61mvxFCKSkpP/30U4vncjgcLpdrMBhYHPrE\ncZzD4bg8rXh+fr7jlTkcDkLIZPpL2GW1jvi1QH4x11tRKbI9xVdqHJFYOzyhVi52rhnHxMQ0\nWc7j8UwmE4u/ylY3BpIkm0yC8/tlHbnESy+9lJubm5mZiRDSaDSDBw8uLi5GCH377bfnz58f\nPHiwUwa1BxRFuedHQhAEQogkSRZ/kxiGYRjG+nyI2565fQPaKND0F2pb6MhH84TGgJr/IlQq\nFUEQIlET+tgkBQUFrTNAr9cz61MoCsstlVzM885WyG0dM3AMxYWokhNrkqIacIxCCDmuqPS4\ns51HbTabWRRoOo5HK34U9huwQwJ98eLFuXPn0tv79u0rLi7es2fPoEGDRo8e/dFHH6Wmpjpl\nUHtAkqTlWFj7geM4j8czGAwsdmB5PB6Px3PP520OiURCURS7NggEgra/yjz66KPfffedVWFy\ncrIjHw3DMLoxuLwD6zhcLhfHcStrz507969//SsnJwfH8X79+n3wwQe9e/du8vTWrShhyM/P\nT0tLKy8vx3DCP2KUT7cZeVVhan0TquIn1Q+Jqx4UU+0tNiCEzCbkuIzRAxr2vxEej6fX69n9\np+Tz+UajsRU/CqlU2twhhwS6srKSWVP0yy+/JCYm0hl05s+f/9lnnzlrDQB4Dv/617/Onj1r\nGSpo6NChL7zwAosmtZEbN27MnDmTHn0iSTIzM3Pq1KlnzpwJDQ1toxxbUVZW9s032/ScbqYu\nT5ulj93j+t8rtq7D45B9I+qGxFXHBCqdjQrXeQaa7eCQQFu+UF+8eHHs2LH0dkBAQFVVVXuZ\nBgDtj1wuT09P/+qrr65cuSIQCJKTk2fPnk0Pqtrn+vXr+/btq6ysDA8PnzNnTnh4uBustaWw\nsJDL5Vr22vbt2zdw4ECranv27Jk6daqrbkpSqKRG/P1RjrLrdxQvxLYCjqHIgMZHYqoHRNcK\nuE73al0lzTk5Od9//z299OaFF16Ijo52yWXdiUMCHR4efvbs2QULFmRmZj548IBJcVZaWurj\n49Oe5gFAuyMWi5csWeLUKbt27XrttdeY3a1bt+7ataudJsztd3ttx9CbDFLhSOSKFjGZsfwK\n2W/35NfvezdouAglIp51HQFV/OQAbGC3Gh9Ja4Z9XNhrTk1Nffnll5nRp2+//fbbb7994okn\nXHV99+CQQM+aNWvlypVlZWU5OTm+vr6jR4+my69evdrcpCoAdFQqKipWrFhhWaLT6f75z39m\nZ2e7JE1UGwcihEJhXV2dVaHjU4W2aAyc28WyG/e9bxV76YxNzKkihHBDCaE8RShPdY8Sju6z\noBV3ce2ARm1t7dKlSy3nBgwGw8svv3zt2rW2PAr345BAL1u2rKamJjU1NSgoaP369fSQdm1t\n7eHDh9988812thAAPIsLFy5otVqrwoqKips3byYlJbXigq4dGk5KSiorK7MtdPY6pbWiW8Ve\nt4q9CislFNX0+DFuKCFU6YTyLK7L++NGzzl1l3YaaL5y5YpKpbIqrK2tzc7ObkUsJxZxSKA5\nHM769evXr19vWejj48OuGywAsEJzDjyOO/a4VpGtSE5OfvDgARPmDCE0bNiw5kILWaHUcnNK\nZbmlstxSWb3GZvziD4K9tX0i6iSmXw//sMHqkONjnu06B9jcd8Giv03rgIwqAOAcffv2tS0U\nCASJiYnNndJqRaYo6sqVK5cvX25oaPDz8xsxYoRlSJwmwXF8zpw5+fn59+7dw3E8JibGTnwM\noxkrqxM9qBYVV4sLKyXl9cLmvHIxjOoW2Ng7vL5X1zp/mR4htGlTmm21c+fO2Y8J4R7fjCb/\nkLhcbq9evdxwdxfSrEBv374dITR79myCIOjt5mBcpAGgMxAfH79o0aKvvvrKsvCdd95hvFld\n2EE+fPgwk7FIqVQqFIopU6YMHTq0xRNjYmKs5oc0ek6DllvXyKtW8atV/Golv0rJr6gXmkl7\n7m8ivikxpKFn14buYQ1i/l/WCjaZxKPJQhp3us117dp1yZIln3zyiWXhihUr/Pz83GaDS2hW\noGlX0OnTpxMEYd8tFAQa6Gy8++67MTExu3fvLi0tjYqKWrx4cffu3V0+cFFVVWWbT+5///tf\nv379LMPVG0x4jYrfqOM06jlqHUet52j0nEYd8fuGntOg4ap1XKPZUT9kDKPC/TQJIcruYfVR\nAWoMa7pT7eXlZZvMUC6XW5Ww5c68fPnyiIiI7du3FxcXR0RELFy4cPLkyaxY0haaFehffvkF\nIcTj8ZhtAABoCIKYO3duSkoKn8/X6XRWYShcxYMHDyx3KY4PyQ018UP3nfc1EV0aNDyVjlun\n5uqb8axwCgyjAuW6mEBVfIgyLlgl4rX8iYYPH65QKGwLmW12V5rgOD5z5syZM2eyaEPbaVag\nLcMGNhdCEAA8EIPB8PDhw8DAwCbjbLQdpqfsEqe65jCTWL0hwOQ1hhLGkPwYM78bIn4PqXP5\nvv1THUIqNAbI9F28tGG+mq5+mlBfDY9Dms1mlUol4MgQsg6jYUtzQxywAtCFwCQh0HGoq6tb\nvXr1/v37TSaTUCh88cUXly5dSr8FuoR29b5ACNWreYoqcVGVpKhK8qBaZDT3R8GtvBSGURKB\nWcQziQUmqcDkJTJIhUaZ0CQXGXylej+pns/9SwwTnU7348H/ZWZmms1mLpc7bNiw0aNH219R\naZWQ8Pz58wghgUAwZcqUVhoN2AACDXQQKIpavHjxmTNn6F2tVvvJJ5/odLp333237RdvJ2mm\nKFTRICwolxRWSvMrpLWNjv6XiPgmb7HRS2TwEpl8pKSIqxULTBK+ScQ3ifkmicAk4ju3wHrP\nnj23bt2it41G45kzZ/R6/dNPP23nFI1Gg/7QZQY7k4RAK2hWoB1P181uSDMAoLl06RKjzgxb\ntmx55ZVXWj133066XFEvzCuT3i2X5lfIVNoWOkkYhgRYjYC87yuq75PgGxXCDfDSMw4VBEFY\nxeJoBQ8ePGDUmeHSpUuPP/647aQfTWRkpEajsVJnhFBcXFxbLAGsaLZxjBs3znL3zp07OTk5\nISEhcXFxGIbl5uaWlpYmJCTY8f0EAHfSZIx5kiQLCwudFej20OXaRl5uqSy3THa3XNagaWHw\n2leij+yijvBv7OqnCfPV/BFvSIqQASHXL7VoMuQZRVFVVVVWAm05vrx8+fL09HTL1WoymczZ\nqCaAfZoV6P379zPbGRkZTz755LZt2+bMmUPHpSZJctu2ba+99trXX3/tDjMBoET5amQAACAA\nSURBVCW8vb2dKrfF5bqs0XNyy6S5pbK8MlmV0t4rKYFT4X7qboGNUV1UkQFqmbCt0caVSmVx\ncTGHwwkNDbWTsIOmufAUTHmT8349e/bctWvX22+/nZOTg2FYUlLS+++/32LSWMApHBqDXr58\n+Zw5cyy9oXEcX7BgQXZ29ooVK9LT09vNPABwlOTkZNv4t3379rUfz8vlomwy44WVktwyWU6p\nrLhabJsglYFDkFEBjTFBqtigxgj/Rh7HZdlAfvnll5MnT9LOf3w+f8KECYMGDbJTv1u3bnK5\n3Gr4uKamZujQoXSHrDmSk5PPnTtHEASGYe3ka9jJcUigr169Onv2bNvyvn372majaAW5ubkr\nVqygKOrQoUNMYVZW1o4dO0pKSuhUwTNmzMCcjfgNdCa8vLy2bNkyf/58JglsZGTk5s2bm2w2\nrtVlisIe1IjoEYzCConR3KyoETgVGaCOC1bGBqmiAho5zSexbjXXrl07duwYs6vX63/88ceA\ngICoqKjmTuHxeLNnz96+ffvPP/9Ml4SFhe3Zs8e+OjPI5XKSJG1D6AFtxyGB5vF4165dsy2/\nevUqn89vowVKpfKjjz7q27evpddOXl7eunXrxowZ8/rrrxcWFm7atIkkyVmzZrXxXkDHZtiw\nYVeuXDl27FhJSUlMTMyYMWOsfOxcqMsUhUprBQWV3jfvCe6WiTWGZn9KGIaCvTXxIaouwvt5\nV/c8+DX3GpfbGBPTZcwYLy8v+3chSfLixYtXrlypq6vz8/NLTk7u27ev/Z7KhQsXbAsvXrxo\nJdBWoxaRkZH9+vUbM2ZMSUlJVFTUU0895bibANB+OCTQ48aN27JlS58+febNm8dk8P3666+3\nbt3aRtGkKOrjjz9OSUkRCASWAp2amhoSErJ48WKEUHh4eHl5eVpa2rRp09r+fwB0bORyOZ2P\nzRIX6nK9mptb5nWnRJZbJlNp7c31eYsNCSHK+BBlfLBSKjTW1NRs2LCBcbfIzMwsKCh44403\nhEKhnYukpaUxgltSUrJr1y6VSpWcnGznlLNnz9I5nS3R6/WrV6+2/9FkMpntowPYxSGB/uij\nj65cubJ48eJVq1bFxMRQFJWfn19TUxMXF/fhhx+25fZ79+41mUzTp0+3HNxACOXk5Fi2wqSk\npH379ikUioSEhLbcDuhUuEqXTWY8v0J6u8Qrp0RWVmdPT0U8U2ywKj5YGR+i7OL1F9e3w4cP\nWznD1dXVnT59mkkgZ0tFRYVtd/jnn38eMGCASCSiu8BcLlcgEFjGPg4JCbEVaJi7+5vikEAH\nBgZevXp1w4YNqampdJzZ6OjoV1555fXXX5dIJK2+9/Xr148dO/bpp59avbJRFFVfX285+U5v\nM2OLCKG6urpRo0Yxu4sWLVq0aFGrLXGWFt9M3QDrb6AcDof12GC20YdzcnKY7bY0ToRQZT3v\nxj3x9XuSnBKRwdTsaCyPQ8YGaxPD1D26asL9tX8M23IQ+svdS0tLbc8tKyuzY+TDhw8tdxmn\n49WrVw8YMMDykOWb5YoVKyZOnGh5VCAQvPnmm+36ZREEwXpjcOGS0VYjkUicbXX2M5E7upJQ\nKpWuWbNmzZo1Tt3bDnV1dR9//PFrr73muBeUJVwu1zIzZlBQkOPh0tsCQRA4jptMJqq5uLnt\nD4ZhOI6zm2Gey+VSFMXuxD2Hw2EMyMvLa8ulKIqqra3lcDgCkXdOifjWA+mN+5KqhmZ/8DhG\nRQdqe3RVJ3bVRAaoOTj5x3VQc19Lk8umORyOne9RIBDYrgRBCHG5XKa12zaGMWPGbNiwYfXq\n1Y2NjQghf3//Tz75pE+fPo78QNRqdUlJSUREhFNjiZ7QGAiCYPcXgeM4bQNJOjfxS5KknaAx\nrC31Lioqqq+vZ5bhUhRFUdSkSZOeeeaZmTNnyuVyy0lhetuyuySRSDZt2sTsajSahoYGN5gt\nEolEIpFarXbP/0GT8Hg8Ho9H//zYws/Pz2w2u+eZN0dlZaVWq237P+Vvv93Y//MtJZZoFvWn\nRDEU1uzIcoCXPiGkISFEGResFHDNXC6Xz+frdHqtvmVtSkhIqKystCqMj4+3yp5lOXfH4XDE\nYrFarbasEBwcHBkZyTx52yEOhNDs2bMnTpx4+/ZtDofTvXt3kUjU4jdVW1v7r3/9a//+/RRF\ncTicuXPnrl692v74OIOvry9Jkuw2BplMplarWdRogUAgkUi0Wm0rVnXa+TtkTaATExM///xz\nZvfUqVM//fTTxo0b6ZVLCQkJ2dnZ8+fPp49mZ2cLBAI7fkJAJ8FyWLmN2T9rGvk5JbLMPPxu\nxTQUML+5anwuGROo7B7W0D20gc4k0jpGjx6dn59vOdDRvXt32j25ufBvfn5+69evf/XVV5lE\nTUKhcNOmTY68y8tkssGDBztoG0VR//znP0+ePEnv0i4AWq32008/dfAKQDvBmkALBILw8HBm\nlx7oYEqmTJny1ltvbdmyZfTo0QqF4uDBg5MmTQIXDk+gvr7+iy++uHnzpkAgGDp06Ny5c9s1\n6iaN49N9Z86cuXjxolarlUqlTz31lFWKI42euFsuyy2T5ZRYLO1r6v0yyFvbPbShe2hDdKCK\nS7hgOIvL5b766qu//vrrvXv3CIKIiYmZPHlyi679U6dO7dGjx969e4uLi6OioubMmRMaGtp2\nY6zIyspi1Jlh165db7zxRlhYmMtvBziOh0azi4uLW7Vq1c6dO48fP+7l5TV58uS/e+DtjkFN\nTc3IkSOZpNFHjhw5cuTIgQMH2iPycit8ML755ps7d+7Q2zqd7rvvvnviiSdSRo0uqpLklMpy\nSmX3HoqbS1CNEEJmJaHOJtS/dgt4+PLTz7Ta8uYgCGLw4MHOtuT4+Pi1a9e63BhLbOPu0xQU\nFIBAs4unCPTkyZOtEtIMGDDAaqoaYJ01a9Yw6kxz8eLF7du3M4NRbaQtjnEKhYJRZ4QQxY80\nifr9L2fA0fK+elOz/x8YZcS1t3B1FtGYievuIkQihOQRDuXAdgp6HOPBgwdpaWkcDmfgwIH+\n/v6OnGg0Gq9cuULn1urfv397rKf19fVtstwRC4uLi0+fPo3jeGJiYkBAgKtN6+x4ikADfwvO\nnTtnW5ient4WgXaVt3JWVhbF8SXF/U3i/qS4P8X5XXTMTU3gBXtrE0KVCcENVYpjh3/ab3XU\n0kGojViOL7/33nubN2+mB5RFItGaNWvmzZtn//ScnJwFCxbcvXuX3h0wYMA333wTFBTkKvNo\nhg4d2rVrV6sMW7169WoxVuUHH3zw3//+l/5EAoHg7bffdqe3a2cABBpwgiZnyZ31K0IuXdqn\nM+L55bLcMtmVxje0MYF2asqExoQQZUKoMj64wUv0uxNOYuigyvL7mZmZ9C6Hwxk1alTbgxrb\nzvvt2bPns88+Y3Y1Gs1bb70VHx8/ZMiQ5i6i1+vnz59vGUY1MzPzpZdeOnjwYBvNs0IoFG7d\nuvWFF15gXo+io6O/+uor+7E49u/fv2HDBmZXp9OtWrUqLi7O/kJHwClAoAEnGDRo0E8//WRV\n6Ii3gGuDE5EUdq9KXFDle/O+qKhKbCabfevHKF1ciKZHWGN8iDLER9tEBQybPn36sGHD7t+/\nTxBEt27d2rLgwk46vm+//da2cPv27XYE+sKFC7ZBri9cuHD37t3Y2NhWG9kkSUlJly5dOnny\nJO0HnZKS0qKvSHOfCATahYBAA07w7rvvnj9/3tJFvVevXgsXLmyysssjeZbXCXPLZLmlsrxy\nqd1U1iSuzSHUVwl11tA+sqlPTWy+5u+Ehoa22jsiOjqacYOzg60TNEKooqLCzilNxtGnz3K5\nQCOERCLRhAkTHK/fik8EOAsINOAEISEhZ8+e/fjjj69fv87j8ZKTk//f//t/TFfLjiLX1tZe\nu3atoaHB39+/f//+jqyAoCjqxo0buYqGOnM3o6BPqTLYfiKSAC9dQnCDsvTkg9sHjbpaoVD4\n+BOPDx8+3NnP2CJMN1koFIrFYqVS6chZYWFhVvOryMKvtLlTmiyPiIhw5I7tTdeuXe/ft04w\n7iG2dRgwFpcsuxCNRkOnsGxv6JWEDQ0NsJLQZDI1GYTWlhs3buzevZt5YhKJ5MUXX7Qz06XS\ncnJKxGmny+rJGJIbYufKMqEpNliZENwQH6L0kbg+FxSyO2rBCLQjPegTJ04899xzliUCgeD4\n8eN2JuLMZvP48eOZ8XGaSZMmbd26ldltciWhezhz5swzz/zFGZHP5x85cqR3797uN8ZDVhI2\nNja2YiWhnVE1EGjn6OQCTfeRzWazRqMhCEIoFLbo9aVSqf7zn/9YtdrAwMClS5danqsz4gUV\nstxSaV6ZrLROZKdV8rlkty6q+BBln2iDn6gOIeuqarW6oaHBx8fH8XhSZrO5urqaw+H4+Pg4\nvmBVo9HU1tb6+/s7uIRq+/bt69ato5dEBwUFffjhh6NHj7Z/SllZ2auvvnr27Fl6d8qUKR99\n9JFMJmMqsCjQCKFdu3atXbuWTsXSpUuX//znP1a5TN0GCLRHAwLdTtiOWly5cuXw4cP00/b3\n9582bVp0dLSdK1y9enX37t225cuWLfPzD7r3UJJX9vsSEjtzfYgy4dqcYGnJM2MiI/0bOQSF\nEBKJRFaxOJRK5f79+2/fvo0QwnF80KBBEyZMaHGhY2Zm5ttvv03HSoyMjFy/fv2IESPsn1JZ\nWbls2bKjR4/SN5o1a9Z7773nyNJzjUaTl5fH5XJjY2Mdj75WXFxcWloaGRnZpUsXq0PsCjRC\nSKvVVlRU4DgeFBTEYjw5EGiPBgTaVdif2bt9+/a2bdssS/h8/htvvNHcSgeEUEZGxoEDB/7c\nx3CKH20SJYV1f7q0IUBvbN6RiyIxfSGhziY0V3HNdYzUxsXFWbrZWgk0SZKbNm2ysn/QoEHT\npk2zvTYzcGH7ni4Wi0+ePNmtW7fm7DKbzZMnT7506ZJl4cyZMzdu3NjsZ2k3WBdo9EewJHZT\nXnVUgYZJws6OU74WthEb9Hr9uXPnrFaBWhIcHIwQonhhZnGSWZREipMowgshpKhuun6Alz4u\nWFlyJ7Ui/zBmbrC9VHMUFBQ02d9/8skn6TGBJkeTP/74Y6sStVr95Zdf2pYzXLp0yUqdEUJ7\n9ux566237FsIAM4CAt3paIv3W01NjW2hVVz5PyureHfLZXnlUabEUQZKbueyMqExLlgZH6yM\nC1b6Sg0IofwA6ZY8peXLHUEQ9kcemrTt3LlzWq3WzrTVvXv3bAubi01h5xSKooqKikCgAdfS\n2QWaUSs7k/XuR6PRXLhwoaKiolu3boMHD25j+AUX+iNLJBKr8MQIIcs5qzo1726Z9G6F7G6Z\ntFplb+pMwDXHBqnigpXxIcpgb+slJFlZWVaDb2az+dq1a3YWQTCZLKyC3NsPEOHn52frz9vi\nKU2WQyQKwOV0doFmsFQxdsU6MzNz4cKFTODgAQMGfPfddw4G1kHtsDzEkkGDBqWlpVkVxvV6\n7EqB391yaX659KHSnihzCDIqoDEhRBUbrIzwV+NYs/MfBQUFtoX5+fnNCXRkZGRAQMDatWut\n0vENGTLEvlfGrFmzVqxYYVVo5Q9nxYgRI2wjVzzyyCMxMTF2zgKAVgAC3QS2Guc2yVYqlfPn\nzy8vL2dKMjMzX3nllT179thWblctbpLhw4dXVFRc+TWT5IWTol5I0ocjH/hVhtTOKThGhfur\n44JVcUHK6MBGLuF04I7miIuLUyqVTCQQsVj89ddfz58/v6SkhC7p2bOnZdqdJpk/f35eXt72\n7dvpXT6fv3LlSvtjKSKRiL4R82fQo0ePzZs3t+WzAECTgEA7BCOFtAeFVqs1m83todonT560\nVGcavV5/7do1OtcMQojD4RAEode3PrtHKzCasQfVYkWltFq+iuwp0hl/d6gyNBUrDsOoMF9t\nTKAyLlgVG6Tic52eW4+Ojr569apVYbdu3ew/86SkpIyMjPT09JKSkpiYmGHDhrUYqBrDsI8+\n+mj+/PlZWVk8Hm/w4MGOREBOSEiYMmXK999/X19fHxAQMHPmTBh9BtoDEOjW0x4dWJVK1eTq\nZJVKxQi029DoCUWVRFEpza+Q3HsoNpntxTbDMCrMVxMT1BgbqOwWqBLx2+TwNGDAgOzsbHoY\nmh5T5nK5X375ZYsnCoXCFld/2BIfHx8fH+94/TfeeOOHH36gtysrK1euXKlSqV5//XVn7wsA\n9gGB9iyadCjGcdwyYW67UtvIK6iQKqokBRXS8johaddLnsDJCH9Nt0BVTKAqOrBR4HxPuTm+\n/vprq9jTRqNxy5Ytq1evdtUtWs1vv/3GqDPD+vXr586d67avCegkdBCBJghCKrU3Etocjq8G\npqEj5PJ4vHZa4NO7d+/w8HCrGDTDhw+3FG4MwzAMc9by5iAprLhakF8uyi8X5ZeL6xpbWHeH\nU1pMfYPQ3sQ01wldbri07/ThsxFCCHERavZcvV6fk5PT0NDQpUuX2NjYJgMNW0Zos8yNwpCT\nk2P5LeM4LpFI3L/SqrCw0LbQaDQWFxfbD37UHuA4juN46xq/q8AwjHUb6AzoLC67owfTBAKB\nsyk67dvcQQSaJElHAtbY4uyCQC6Xi+O4yWRqRZR6B5k7d+6+fftyc3MRQjiODxkyZNy4cZZ2\nEgSB47it5QaDobKyksvl+vv72x97VeuJoipJYYW4oEJ876HY3nI+hBBC3mJjt6DGbl0aH+T8\ndO3ij3ReKJrLly/HxsYmJSXZOf3evXvbt2+nIzYghEJDQxcsWMCM2DArxS2XYInFYtvriEQi\nyzocDkev17ffF9EczS1o5vF4rVhF1kY4HA4r97WE7q+wawNBEAaDgcWVhDwej8PhGI3GVgiR\nnc5WBxFoiqJat/ba2W+UFj6SJNuvKchksoULFyqVSjo4J/3lWd6O7kFbGXD+/PmjR4/SM4c+\nPj7Tpk2z7JBSFFZWJyiqkhRVSRRV4soGof2uBoahQLm2W2BjdBdVt8BGX8nvE5LrDpywVGea\n69ev21kJotPpLNUZIVRSUrJmzRomLUiTX9y4ceMs848whZaV6S/d/QI9dOhQLy8vOuYRQ1RU\nVGxsLCsBAGhdcP99LWn1D9BVkCRpNBpZFGhaGcxms2ufQwcR6I6HTCazXABin+vXrx86dIjZ\nra2t3b59+/x/rFSaw4qqxPceSoqrxbqWuslcggz3V3cLbIzu0hgV0CjiN+Gf0WTvwH6X4e7d\nu4w6Wy4huXfvnp3YwW+++ebly5d//fVXpuS5556zs6Dcnfj4+GzcuPHFF19k+oze3t5btmxp\nj9TmQCcHBLojcOrUKYQQxfEjBbGkII4UxmsEceuPtzxhJRMao7o0dgtsjAxo7OqrpqPE2SE0\nNDQvL8+qMCTEXshmlUpltbSP5uHDh3YEms/nf/3116+88srNmzd5PN7w4cPff/99+7a5k7Fj\nx2ZkZKSlpVVUVHTt2nXq1KkwPQi0ByDQf2NqVLySWvGDatF97hJjTDSTx9oOOEaF+mqiuqgj\n/RujujT6SZ1zph4/frxCobB8ifP29n700UdtazIOy01GrsBx3L5Hc3l5+WOPPcaE1/jhhx9y\nc3OPHj3KYkBLK8LCwpYtW+Z4wH4AaAUg0H8bSAqrqBeU1IjK6iXF1aJ7DwUa/R9fn9DeKglv\nsSEyQB3h3xgZoO7qp+ZxWj9oGxQU9NJLLx05cuTBgwd0UOOxY8da5q+yld3hw4cPHDjQcrAC\nITRnzhz7uVnXrl1rFfzoxo0bW7Zsefnll1ttPAD87QCB9lw0eqK0VlRaJyquFpbUisrqhPaX\nijBg5gZCl/dIT1nPaCLCX+0lcuWsRdeuXf/xj39IJBKSJOkY3Pb7whwO55tvvlm2bNmxY8fo\n3blz565Zs8b+XWzjeSKEMjIyQKCBTkWnFmjaSatJn1z3YyaxqgZBaa2wtE5UVissrRPVqBx9\nnZcITHxzkbLiV1yXh2vzBHjtxIkTH3nkEQdP1+v1DiZtsiQuLs7SN8MOgYGBO3bsqKurKysr\ni4iIaNKFzoomvxQP+aYAwG10UoE+ffr0O++8k5ubm5ycHBcXN2HCBPtv3C6HpFCNil9WJyyr\nE5bVicrrhJUNApPZ0bCivlJjqI861Ffd1VcT6quh86VWV/s8eBDC4YRHR0c7IoI6ne7YsWOZ\nmZk6nc7Ly+uxxx4bNmyY/dCmTGeZThrroLU03t7e3t7eDlYePnz43r17rQrtxBoFgA5JZxTo\ny5cvP/vss/S2yWS6fft2WVnZ66+/7khOudZBUli1il9eJyivE1bUC8vrheV1AqNj4xUIIQKn\nAuW6UF9NqI8m1FcT2cUgE1G2wZL8/Pwc/5uhKGr37t10+j6EUENDw6FDhwwGw+OPP25Vk5Xg\nq2vXrj137lxZWRlT8sgjj8ybN8/9lgAAi3RGgX7nnXesSurq6s6fP//kk0+65Po6I17VIKxs\nEJTXCSobBJUNzvWOEUJSoTHERxvirQn11Yb4aILkWksHOA6Hg1BbXW4VCgWjzgwnTpwYOnSo\nQCBgPX2Br69venr6F198kZWVxeVyR44cOW/ePA6nMzZXoDPTGVt8k3EebIN8OgJJYdVK3kOl\noKJBUNUgqGoQVDYI6tTOuYIJuGSgXBvsrQn20Yb4aEN8tFJBuy/Ksv28tLfy8uXLExIS2vvu\njiCXy1etWsW2FQDAJp1RoKVSqW0K8BZjD5lJrKaRX6cR16pFZdW+FfX8KiW/RsU3k87lo+Jz\nyS5e2iC5NthHFyjXhnhrfST6tuW0ag3M57VaReL48kUAANqbzijQkyZN2rJli1Vhnz59mG21\nnlOj4lereDUqwUMl/6GKX63k1zbySMppHRXzTV3kuiC5touXLthbGyjXsSLHVkRGRkokkuXL\nlyuVSsvynj17MqGLAABgnc4o0KtWrcrOzs7MzMQ5YlIQS3EDoxOH3azve/YXWpT5OmNrRnhx\njPKTGQJkui5e2i5yXaCXLtBb54bBCgexGlb29/ffuHHjSy+9pNX+nrA1MDBw8+bNbUxQCwCA\nC+mMAi0UCg8fPnzkyJHTWdoc00sIoTtKdOeWE1fAMCQXGfxlui5eej+ZrouXvouX1k+qbzGW\nhSNQFHXlypWsrCylUhkQEPDYY4+1uldrf65v3Lhxffv2PXToUFlZWWxs7NSpUx1xzgMAwG10\nRoFGCOE4Pn78+D6DiIWftlCTQ1A+Yr2fTO8v0/vL9EE+5hBfUsZXYqi9usaHDh26cOECvV1T\nU5OTk/P888/biedpibPeFyEhIf/85z+dNhEAALfQSQUaIZSRkZF59SaGraIjI2MY8hIZfSV6\nX6neV6L3kxl8JXo/qc5HYsSwP/vFfySNJdsp8GxJSQmjzgwHDhzo3r17c05mrLvEAQDQTnRG\ngTYYDC+88MKJEycQQoPHURyq/tHBcWMe7+uSAYo20mTsN7VaXVVVZZk3OjY2trGx0X1mAQDA\nBp4r0FlZWTt27CgpKfHy8kpJSZkxY4ar5q/Wr19PqzNCiNNwAiF09vividFST3BgaC7chGV8\nTs8JuQkAQLviodFn8vLy1q1bl5iYuGHDhlmzZqWmpu7atctVF7dNyYwQunr1qquu3xZiYmKs\nSs6fP19UVDRw4EBW7AEAgEU8tAedmpoaEhKyePFihFB4eHh5eXlaWtq0adNaEXTNlrq6OttC\ntVrd9iu3HX9//6eeemrFihVMCZ/P/+9//wvplACgE+KhAp2Tk2MZuiwpKWnfvn0KhYJZhWw2\nm+/evctUkEqlEonEwYvHxMRcv37dqjAwMNCRaJb0MAuO4y5P8M4MsMTExCQkJOzdu7e8vDwm\nJubFF1+0Gnuhs3qzHpgCwzB2baANcH/SWAa6wRAEweJzIAiC9S8CeUZjoB8FWwbQjaEVP0z7\nSuKJAk1RVH19vWVoSnq7traWKVEqlbNnz2Z2Fy1atGjRIgev//77748dO9ayRCqVPvnkk45H\ns3NJR56mycAXEydOnDhxov0TWR+JJghCLpeza4MnLEz3BOdx1hsDhmGsNwYul8uuAQghkUjk\nbFBM+5nIPVGgHYHP50+ZMoXZjY2NZVIst8jIkSO3b9++cuVKOpplZGTk9OnThUKhI/nS6d6r\nyWRqYw86NjaW3nDcbAYcxwmCYDfLvUAgIEmS3Vx8PB7PaDS6/FXGcTgcDofDMRqN9n9j7Yon\nNAa6v2Ib/9adsN4YCILgcrmtaAwkSdrRdE8UaPrf2HKkmN62TJwsEolWrlzJ7Go0GqfczsaO\nHTt27NiysrK6ujr66TjYvGg/6Fb/JhlPjLY4ydE2sOtmRws0uzbI5XK1Ws3iEIdQKORwOFqt\nlsU/Ki6XKxAI2P0i+Hw+641BJpNpNBoW/ykFAgGXy9Xr9a3octkRaA/14khISMjOzmZ2s7Oz\nBQJBVFSUa+8SHBzcfkH6LYn8AzfcCwCADoOHCvSUKVNKS0u3bNly//79M2fOHDx4cMKECS4c\n+XUPoMsAALQFTxziQAjFxcWtWrVq586dx48f9/Lymjx58syZM9k2ylFAkQEAcAkeKtAIoQED\nBgwYMIBtK5wAdBkAANfiuQL9NwKkGQCA9gAEuvWALgMA0K6AQLeG6Ohodj1PAQDoDIBAO0Fk\nZCS9UqihoYFtWwAA6PiAQDsEjGYAAOB+QKBbAKQZAAC2AIFuGtBlAABYBwTaGpBmAAA8BBDo\nPwFpBgDAowCBRgikGQAAj6SzCzRIMwAAHouHRrMDAAAAQKABAAA8FBBoAAAADwUEGgAAwEMB\ngQYAAPBQQKABAAA8FIzFROUuxGAw4Lg7/mxwHMdx3Gw2s/jcMAzDMIzFbNYIIQ6HQ1EUi0mU\nEUIEQbBrgIc0BtoGtgxAHtMYSJJk8YugGwNJks7+MEmS5PF4zR3tIH7QJpNJq9W64UZ37tzJ\ny8sbPny4n5+fG27XJARBEARhMBjYMgAhdOjQIW9v7+TkZBZtEAgEer2eYKnQ/wAACORJREFU\nxd9kQUHBzZs3Bw0aFBQUxJYNntAYDh8+LBAIUlJSWLSBz+cbjUYWey3379/Pzs7u379/WFiY\ns+f6+vo2d6iDCDQdptkNN7p58+a2bdt69eoVFxfnhtt5LFu2bElISJgyZQq7ZojFYhbvfuTI\nkc2bN0dFRfXo0YNFM1jnu+++8/b2fvbZZ9k2hE3S09M3b968evXqPn36uPCyMAYNAADgoYBA\nAwAAeCgg0AAAAB5KB/HicBt6vV6v14vFYoIg2LaFTZRKJYfDcc+4v8diMBh0Op1IJOJwOshc\nTutQqVQ4jrM7H8A6dGMQCoVcLteFlwWBBgAA8FBgiAMAAMBDAYEGAADwUDr12JlTHDlyZMuW\nLZYl7733Xu/evdmyx23cvXv3wIEDhYWFVVVVo0aNevnlly2PZmVl7dixo6SkxMvLKyUlZcaM\nGRiGsWVq+2HnIXSqhnHy5Mn09PR79+7p9frg4OCxY8eOGjWKOdpJGoOdh+DyxgAC7QRSqfS9\n995jdoODg1k0xm3odLqgoKAhQ4bs3r3b6lBeXt66devGjBnz+uuvFxYWbtq0iSTJWbNmsWJn\nu2LnIaDO1DBOnz7dvXv3iRMnikSijIyMzz//3GQyjRkzBnWmxmDnISBXNwYQaCcgCCIqKopt\nK9xNr169evXqhRBKTU21OpSamhoSErJ48WKEUHh4eHl5eVpa2rRp0/h8PguGtid2HgLqTA3j\n/fffZ7YTExOLioouXrxIa1PnaQx2HgJydWMAgXYClUr1/PPPm0ym0NDQiRMnDh06lG2LWCYn\nJ8cyHEdSUtK+ffsUCkVCQgKLVrmfTtswDAZDQEAAvd1pG4PlQ0Cubgwg0I4SFhb2j3/8Izw8\n3GAwpKen/9///d+CBQsmTJjAtl2sQVFUfX29t7c3U0Jv19bWsmcUC3TahnHy5MmCgoJFixah\nTtwYLB8CaofGAALtKMxLLkKoZ8+earX6wIEDneF3CNinczaM8+fPb968ecmSJTExMWzbwhq2\nD8HljQHc7FpJQkJCXV2dyWRi2xDWwDBMLpfX1dUxJfS2j48Pe0axT2doGEePHt24cePSpUtH\njBhBl3TCxmD7EGxpe2MAgW4lOTk5crm8k6/xTUhIyM7OZnazs7MFAkEnmS5rjg7fMPbu3fvt\nt9++/fbbgwYNsizvVI2huYdgRdsbQ4dtRi7niy++SEhICAoKMhgM586du3jx4gsvvMC2Ue7A\nYDCUlJTQG42NjQqFAsOwyMhIhNCUKVPeeuutLVu2jB49WqFQHDx4cNKkSR1v1h7ZfQidqmFs\n3br1559/XrRokVQqVSgUCCEul0uHqO88jcHOQ3B5Y4BYHI6ydevWrKysmpoaHo8XEhIyYcKE\n4cOHs22UO1AoFK+99pplCY7jhw4dorczMzN37txZXFxMr02YOXNmh1ybYOchdKqG8dxzz6lU\nKsuSwMDAr776it7uJI3BzkNweWMAgQYAAPBQYAwaAADAQwGBBgAA8FBAoAEAADwUEGgAAAAP\nBQQaAADAQwGBBgAA8FBAoAEAADwUEGgAcB9Tp04VCARsWwH8bQCBBgAA8FBAoAEAADwUEGgA\nAAAPBQQa6ICYTKYPP/ywZ8+eUqlUKpXGxMTMnTuXCXCzf/9+DMP27t27atWqiIgIPp8fExPz\n6aef2l5kw4YNffr0EQqFUqn00UcfPXHihFMVKisr58yZ4+PjIxaLk5OTMzIy2vVTAx0PCDcK\ndEBWrFixfv36mTNnvvLKKziO379///Dhw0qlUiqVMnWWLl3ar1+//fv3SySS7du3L1mypLKy\n8oMPPqCPms3mCRMmHD9+fNq0aQsWLNDpdDt37hw9evSuXbtmzJjhSIXGxsbk5OT8/PyFCxf2\n69fv2rVrTzzxRNeuXVl5IMDfFQoAOhyRkZGPPfZYc0d//PFHhFBkZKTRaGQKp0+fjuN4fn4+\nvfvFF18ghLZt28ZUMBgMSUlJXbp0oc9qscI777yDEPryyy+ZClu3bkUI8fl8131QoIMDQxxA\nB0Qul+fk5GRmZtqpM3fuXMtUFwsXLiRJkolz/f333wcEBMyYMUP3B2azecaMGZWVldevX3ek\nwoEDB3x9fRcsWMDcYt68eSEhIe3ygYEOCgxxAB2Q9evXP/PMMwMHDuzateuwYcNSUlKeffZZ\nkUhkWSc6Otpyl07OVFhYSO/m5OQolUqhUGh78aqqKkcqFBYW9uzZ0/I/AMfx+Pj4CxcutPXj\nAZ0GEGigAzJy5MiioqJjx46dOXMmPT199+7da9asuXTpkmUHVq/XW55C7zIZQEiSjImJ+f77\n720vHh8f70gFy6sxUJAfA3AGEGigYyKVSqdNmzZt2jSE0N69e2fMmPHZZ5/93//9H1Ph1q1b\nlvXpXSbJaWxs7K1bt3r06CGRSJq8fosVoqOj8/PzTSYT04kmSTIvL6/NnwzoRMAYNNABqa2t\ntdylUy9bFX777bcVFRX0ttFo/PjjjzEMmzhxIl3y/PPPGwyGpUuXWvV5y8rKHKwwZcqU6urq\nbdu2MYe+++670tLStn86oPMAOQmBDohAIBg3bly/fv1CQkKqqqq+/vrrgoKC06dPjxgxAiG0\nf//+adOm9evX7+HDhy+++KJEItm9e/fly5eXLVv24Ycf0lcwmUyTJ08+fPjwwIEDJ06c6O/v\nX1xcfOnSpevXr9NDzC1WUKlU/fr1KywsXLx4cd++fa9fv/7dd9+FhYUpFAqdTsfiwwH+TrDr\nRAIA7cHKlSuHDBni5+fH5XJDQkImTZqUkZHBHKXd7A4cOPDhhx9GRUXxeLzo6OiPP/6YJEnL\ni5jN5s2bNw8aNEgikQgEgoiIiEmTJu3YscPxCuXl5bNmzZLL5SKRaPjw4RcvXnz66afBzQ5w\nHOhBA50Ougd98ODBSZMmsW0LANgDxqABAAA8FBBoAAAADwUEGgAAwEOBMWgAAAAPBXrQAAAA\nHgoINAAAgIcCAg0AAOChgEADAAB4KCDQAAAAHgoINAAAgIcCAg0AAOChgEADAAB4KCDQAAAA\nHsr/B8VmqlHbcFS/AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, lets visually examine the data\n",
    "\n",
    "# load the required packages for plotting\n",
    "library(tidyverse)\n",
    "# set plot dimensions\n",
    "options(repr.plot.width=4, repr.plot.height=2)\n",
    "\n",
    "# draw the scatter plot between 'speed' and 'dist'\n",
    "ggplot(data = cars, mapping = aes(x = speed, y = dist)) +\n",
    "    geom_point() +\n",
    "    geom_smooth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above plot suggests that 'dist' can be computed from 'speed' through a linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "cars_lm <- lm(dist ~ speed, data = cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the fitted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## summary of fitted model\n",
    "summary(cars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How fit is the fitted model? Refer to this [blog](https://boostedml.com/2019/06/linear-regression-in-r-interpreting-summarylm.html) for the intepretation of the summary of the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the fitted model for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example unseen data\n",
    "df <- data.frame('speed' = c(2,3,4,7))\n",
    "# prediction\n",
    "predict(cars_lm, newdata = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will use another dataset that comes with R, `mtcars`, to build a model with **multiple features** to predict the fuel consumption `mpg.` The features describe different aspects of an automobile design and performance. We will also explore **which features to use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset mtcars to create a linear regression model to predict `mpg` using `wt, qsec, am` and `carb`. \n",
    "mtcars_lm <- lm(mpg ~ wt + qsec + am + carb, data = mtcars)\n",
    "summary(mtcars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Using the summary output of the model, to determine\n",
    "\n",
    "    - Does the fitted model fits the dataset well?\n",
    "    - What is the mathematical form of the model?\n",
    "    - Is the coefficient for 'wt significant?\n",
    "    - Is the coefficient for carb significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the dataset mtcars to create a linear regression model to predict `mpg` using all features. Remove the top 3 least significant features from the dataset one by one to reduce the model without degrading the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "## fit model\n",
    "mtcars_lm <- lm(mpg ~ ., data = mtcars)\n",
    "summary(mtcars_lm)\n",
    "\n",
    "## remove cyl, vs, gear\n",
    "mtcars_lm <- lm(mpg ~ drat + wt + qsec + am + carb + disp + hp, data = mtcars)\n",
    "summary(mtcars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of determining label(s) (e.g., categories or clasesses, etc.) of each data observation based on learning from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quizz** which of the following tasks are classification problems?\n",
    "\n",
    "    ☑ Deciding whether to buy, sell or hold a stock.\n",
    "    ☑ Predicting the winner of an election with six parties.\n",
    "    ☑ Telling whether or not revenue will exceed $100,000.\n",
    "    ☐ Forecasting the weekly revenue of a company.\n",
    "    ☐ Predicting the number of daily bike thefts in a city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widely used (shallow) classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Logistic Regression**](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "Provided in `stats` package, which is automatically loaded when starting R  \n",
    "\n",
    "    glm_model <- glm(y ∼ x1 + x2, family = binomial, data = mydata)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**K-Nearest Neighbor**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "Install and load the `class` package\n",
    "\n",
    "    knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n",
    "    \n",
    "`knn_model` is a factor vector of class attributes for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Decision Trees (CART)**](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "Install and load the `rpart` package.\n",
    "\n",
    "    cart_model <- rpart(y ∼ x1 + x2, data=mydata, method=\"class\")\n",
    " \n",
    "You can use `plot.rpart` and `text.rpart` to plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "Install and load the `randomForest` package\n",
    "\n",
    "\n",
    "    rf_model <- randomForest(y ~ x1 + x2, data=train, importance=TRUE, ntree=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Support Vector Machines (SVM)**](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "Install and load the `e1071` package.\n",
    "\n",
    "    svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will use the above models to predict `survivors` in the [Titanic dataset](https://www.kaggle.com/c/titanic). The dataset also provided under `data/titanic.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, view data examples, and summarize the dataset\n",
    "titanic <- read_csv('data/titanic.csv', skip=5)\n",
    "head(titanic)\n",
    "summary(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: most models do not work with `character` type, we need to convert strings to factors for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic$Sex <- as.factor(titanic$Sex)\n",
    "titanic$Cabin <- as.factor(titanic$Cabin)\n",
    "titanic$Embarked <- as.factor(titanic$Embarked)\n",
    "titanic$Survived <- as.factor(titanic$Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Testing data**\n",
    "\n",
    "split the titanic data into training and testing sets based on the feature we want to predict `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_training <- filter(titanic, !is.na(Survived))\n",
    "dim(titanic_training)\n",
    "titanic_testing <- filter(titanic, is.na(Survived))\n",
    "dim(titanic_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is missing value in training data\n",
    "summary(titanic_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is missing value in test data\n",
    "summary(titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting `Survived` based on `Pclass` and `Sex` using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "titanic_glm <- glm(Survived ~ Pclass + Sex, data = titanic_training, family = binomial)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: the coefficient for Sexmale is significantly negative. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained model for predict the survivor in test set\n",
    "titanic_testing$Survived <- predict(titanic_glm, titanic_testing, type=\"response\") # Question: why type=\"response\"\n",
    "titanic_testing$Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**: train a `random forest` for predicting  `Survived` based on `Pclass` and `Sex`, then apply the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in eval(m$data, parent.frame()): object 'titanic_training' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(m$data, parent.frame()): object 'titanic_training' not found\nTraceback:\n",
      "1. randomForest(Survived ~ Pclass + Sex, data = titanic_training, \n .     importance = TRUE, ntree = 10)",
      "2. randomForest.formula(Survived ~ Pclass + Sex, data = titanic_training, \n .     importance = TRUE, ntree = 10)",
      "3. eval(m$data, parent.frame())",
      "4. eval(m$data, parent.frame())"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# install.packages('randomForest')\n",
    "library(randomForest)\n",
    "\n",
    "# train the model\n",
    "titanic_rf <- randomForest(Survived ~ Pclass + Sex, data = titanic_training, importance=TRUE, ntree=10)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_rf)\n",
    "\n",
    "# Apply the trained model for predict the survivor in test set\n",
    "\n",
    "titanic_testing$Survived <- predict(titanic_rf, titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "#### Measures of goodness\n",
    "We trained and applied some models to predict labels for un-labeled data, but how can we say if the model is good?\n",
    "\n",
    "The goodness of prediction models are measured w.r.t some dataset with groudtruth -- i.e., we need the labels for observations in test data. Typical measures for the goodness of the classification models are [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F1 scores](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will train a model and evaluate its performance using [Iris dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  X1 = col_double(),\n",
      "  X2 = col_double(),\n",
      "  X3 = col_double(),\n",
      "  X4 = col_double(),\n",
      "  X5 = col_character()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "iris <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', col_names = FALSE)\n",
    "\n",
    "# name the columns\n",
    "names(iris) <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>sepal_length</th><th scope=col>sepal_width</th><th scope=col>petal_length</th><th scope=col>petal_width</th><th scope=col>class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1        </td><td>3.5        </td><td>1.4        </td><td>0.2        </td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.9        </td><td>3.0        </td><td>1.4        </td><td>0.2        </td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.7        </td><td>3.2        </td><td>1.3        </td><td>0.2        </td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.6        </td><td>3.1        </td><td>1.5        </td><td>0.2        </td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.0        </td><td>3.6        </td><td>1.4        </td><td>0.2        </td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.4        </td><td>3.9        </td><td>1.7        </td><td>0.4        </td><td>Iris-setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " sepal\\_length & sepal\\_width & petal\\_length & petal\\_width & class\\\\\n",
       "\\hline\n",
       "\t 5.1         & 3.5         & 1.4         & 0.2         & Iris-setosa\\\\\n",
       "\t 4.9         & 3.0         & 1.4         & 0.2         & Iris-setosa\\\\\n",
       "\t 4.7         & 3.2         & 1.3         & 0.2         & Iris-setosa\\\\\n",
       "\t 4.6         & 3.1         & 1.5         & 0.2         & Iris-setosa\\\\\n",
       "\t 5.0         & 3.6         & 1.4         & 0.2         & Iris-setosa\\\\\n",
       "\t 5.4         & 3.9         & 1.7         & 0.4         & Iris-setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "sepal_length | sepal_width | petal_length | petal_width | class | \n",
       "|---|---|---|---|---|---|\n",
       "| 5.1         | 3.5         | 1.4         | 0.2         | Iris-setosa | \n",
       "| 4.9         | 3.0         | 1.4         | 0.2         | Iris-setosa | \n",
       "| 4.7         | 3.2         | 1.3         | 0.2         | Iris-setosa | \n",
       "| 4.6         | 3.1         | 1.5         | 0.2         | Iris-setosa | \n",
       "| 5.0         | 3.6         | 1.4         | 0.2         | Iris-setosa | \n",
       "| 5.4         | 3.9         | 1.7         | 0.4         | Iris-setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  sepal_length sepal_width petal_length petal_width class      \n",
       "1 5.1          3.5         1.4          0.2         Iris-setosa\n",
       "2 4.9          3.0         1.4          0.2         Iris-setosa\n",
       "3 4.7          3.2         1.3          0.2         Iris-setosa\n",
       "4 4.6          3.1         1.5          0.2         Iris-setosa\n",
       "5 5.0          3.6         1.4          0.2         Iris-setosa\n",
       "6 5.4          3.9         1.7          0.4         Iris-setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes ‘tbl_df’, ‘tbl’ and 'data.frame':\t150 obs. of  5 variables:\n",
      " $ sepal_length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
      " $ sepal_width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
      " $ petal_length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
      " $ petal_width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
      " $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n",
      " - attr(*, \"spec\")=List of 2\n",
      "  ..$ cols   :List of 5\n",
      "  .. ..$ X1: list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X2: list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X3: list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X4: list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X5: list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_character\" \"collector\"\n",
      "  ..$ default: list()\n",
      "  .. ..- attr(*, \"class\")= chr  \"collector_guess\" \"collector\"\n",
      "  ..- attr(*, \"class\")= chr \"col_spec\"\n"
     ]
    }
   ],
   "source": [
    "# view some rows\n",
    "head(iris)\n",
    "\n",
    "# view columns's data type\n",
    "str(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `class` from string to factor\n",
    "iris$class <- as.factor(iris$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models, often we **divide** the set of labeled data into **training and test sets**. The models will be **trained on the training set**, and **evaluated using the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the iris dataset into: 80% for training set and the remaining 20% for test set \n",
    "training_size <- floor(0.8 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a svm model to predict `class` from `sepal_length` and `sepal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'e1071' package if not yet\n",
    "# install.packages('e1071')\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the trained model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'mltest' package if not yet\n",
    "# install.packages('mltest', repos = 'https://cran.r-project.org/')\n",
    "library(mltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.8"
      ],
      "text/latex": [
       "0.8"
      ],
      "text/markdown": [
       "0.8"
      ],
      "text/plain": [
       "[1] 0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overall classification accuracy\n",
    "classifier_metrics$accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.666666666666667</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.625</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.666666666666667\n",
       "\\item[Iris-virginica] 0.625\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.666666666666667Iris-virginica\n",
       ":   0.625\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6666667       0.6250000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# precision for classes\n",
    "classifier_metrics$precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.666666666666667</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.625</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.666666666666667\n",
       "\\item[Iris-virginica] 0.625\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.666666666666667Iris-virginica\n",
       ":   0.625\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6666667       0.6250000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recall for classes\n",
    "classifier_metrics$recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.666666666666667</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.625</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.666666666666667\n",
       "\\item[Iris-virginica] 0.625\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.666666666666667Iris-virginica\n",
       ":   0.625\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6666667       0.6250000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** divide the iris dataset into training and test sets by ratio 9:1, train a svm model to predict `class` using all feature, then examine the performance of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.96551724137931</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.954545454545455</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.96551724137931\n",
       "\\item[Iris-virginica] 0.954545454545455\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.96551724137931Iris-virginica\n",
       ":   0.954545454545455\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9655172       0.9545455 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.96551724137931</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.954545454545455</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.96551724137931\n",
       "\\item[Iris-virginica] 0.954545454545455\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.96551724137931Iris-virginica\n",
       ":   0.954545454545455\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9655172       0.9545455 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.96551724137931</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.954545454545455</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.96551724137931\n",
       "\\item[Iris-virginica] 0.954545454545455\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.96551724137931Iris-virginica\n",
       ":   0.954545454545455\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9655172       0.9545455 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "#divide the iris dataset into training and test sets by ratio 50:50\n",
    "training_size <- floor(0.5 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)\n",
    "\n",
    "\n",
    "# precision for classes\n",
    "classifier_metrics$precision\n",
    "# recall for classes\n",
    "classifier_metrics$recall\n",
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "Underfitting happens when the trained model does not fit well with the training data, and thus often lead to poor performance on test data\n",
    "\n",
    "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) happens when the trained model fit too well with the training data while perform (predict) poorly on test data\n",
    "\n",
    "\n",
    "**Example**: Use the above excercise, change the to ratio 5:95 and 10:90, and change the number of tree (`ntree`) in the model to 1 and 10. Examine the performance of the trained model on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>1</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 1\n",
       "\\item[Iris-virginica] 1\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   1Iris-virginica\n",
       ":   1\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "              1               1               1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Iris-setosa</dt>\n",
       "\t\t<dd>0.912621359223301</dd>\n",
       "\t<dt>Iris-versicolor</dt>\n",
       "\t\t<dd>0.634615384615385</dd>\n",
       "\t<dt>Iris-virginica</dt>\n",
       "\t\t<dd>0.556962025316456</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 0.912621359223301\n",
       "\\item[Iris-versicolor] 0.634615384615385\n",
       "\\item[Iris-virginica] 0.556962025316456\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   0.912621359223301Iris-versicolor\n",
       ":   0.634615384615385Iris-virginica\n",
       ":   0.556962025316456\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      0.9126214       0.6346154       0.5569620 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "training_size <- floor(0.05 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# train a random forest model\n",
    "iris_rf <- randomForest(class ~ sepal_length + sepal_width, data = iris_train, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_train$class)\n",
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(iris_rf, iris_train))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)\n",
    "# F1-measures for classes\n",
    "classifier_metrics$F1\n",
    "\n",
    "\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(iris_rf, iris_test))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)\n",
    "# F1-measures for classes\n",
    "classifier_metrics$F1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "As we see in the previous section, the performance of models are highly dependent of the datasets. Hence, their performance can be high or low by chance, e.g., by the way we divide the original labeled dataset into training and test set. To reduce the effect of the chance, or to get more robust, reliable measures for the performance, we need to run K-fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "Basically, K-fold cross validation includes:\n",
    "\n",
    "- Devide the labeled dataset in to K equal folds\n",
    "- In turn, take one fold as test set, the union of other K-1 folds is used as train set\n",
    "- Train a model for each division of training & test sets, i.e., K models in total\n",
    "- Evaluate each model on corresponding test set\n",
    "- Aggregate the peformance of K models to form the final performance\n",
    "\n",
    "The prediction is for a new data observation is then formed by aggreating (e.g., voting base) the predictions of the above K models\n",
    "\n",
    "Refer to `trainControl` object and `train` function in `caret` package for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "One important aspect of the shallow classification models is the importance score of features in (trained) models. These scores inform us how important the features are in predicting the label of data observations.\n",
    "\n",
    "**Example** we will examine the importance of features in `random forest` and `svm` (with `linear kernel`) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the importance scores of features in `random forest` model is quite simple\n",
    "\n",
    "# train a random forest model\n",
    "iris_rf <- randomForest(class ~ ., data = iris, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(iris_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a bit more complicated with SVM-linear model\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris[features], y=iris$class, kernel =\"linear\", cost=1)\n",
    "w <- t(svm_model$coefs) %*% svm_model$SV        # weight vectors\n",
    "w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight\n",
    "w <- sort(w, decreasing = T)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using [US's income dataset](https://archive.ics.uci.edu/ml/datasets/Adult), train a random forest model for predicting `income` from all other features, examine the importance of the features for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  X1 = col_integer(),\n",
      "  X2 = col_character(),\n",
      "  X3 = col_integer(),\n",
      "  X4 = col_character(),\n",
      "  X5 = col_integer(),\n",
      "  X6 = col_character(),\n",
      "  X7 = col_character(),\n",
      "  X8 = col_character(),\n",
      "  X9 = col_character(),\n",
      "  X10 = col_character(),\n",
      "  X11 = col_integer(),\n",
      "  X12 = col_integer(),\n",
      "  X13 = col_integer(),\n",
      "  X14 = col_character(),\n",
      "  X15 = col_character()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Feature</th><th scope=col>Importance</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>age           </td><td> 3.5945641    </td></tr>\n",
       "\t<tr><td>workclass     </td><td> 6.2591816    </td></tr>\n",
       "\t<tr><td>fnlwgt        </td><td> 0.3867257    </td></tr>\n",
       "\t<tr><td>education     </td><td> 3.2916342    </td></tr>\n",
       "\t<tr><td>education_num </td><td> 3.0059581    </td></tr>\n",
       "\t<tr><td>marital_status</td><td> 6.1889564    </td></tr>\n",
       "\t<tr><td>occupation    </td><td> 6.8541051    </td></tr>\n",
       "\t<tr><td>relationship  </td><td> 4.7061863    </td></tr>\n",
       "\t<tr><td>race          </td><td> 3.2044790    </td></tr>\n",
       "\t<tr><td>sex           </td><td> 3.8744382    </td></tr>\n",
       "\t<tr><td>capital_gain  </td><td>25.6073567    </td></tr>\n",
       "\t<tr><td>capital_loss  </td><td>12.8919560    </td></tr>\n",
       "\t<tr><td>hours_per_week</td><td> 5.0180808    </td></tr>\n",
       "\t<tr><td>native_country</td><td>-2.1015123    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Feature & Importance\\\\\n",
       "\\hline\n",
       "\t age            &  3.5945641    \\\\\n",
       "\t workclass      &  6.2591816    \\\\\n",
       "\t fnlwgt         &  0.3867257    \\\\\n",
       "\t education      &  3.2916342    \\\\\n",
       "\t education\\_num  &  3.0059581      \\\\\n",
       "\t marital\\_status &  6.1889564      \\\\\n",
       "\t occupation     &  6.8541051    \\\\\n",
       "\t relationship   &  4.7061863    \\\\\n",
       "\t race           &  3.2044790    \\\\\n",
       "\t sex            &  3.8744382    \\\\\n",
       "\t capital\\_gain   & 25.6073567      \\\\\n",
       "\t capital\\_loss   & 12.8919560      \\\\\n",
       "\t hours\\_per\\_week &  5.0180808        \\\\\n",
       "\t native\\_country & -2.1015123      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Feature | Importance | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| age            |  3.5945641     | \n",
       "| workclass      |  6.2591816     | \n",
       "| fnlwgt         |  0.3867257     | \n",
       "| education      |  3.2916342     | \n",
       "| education_num  |  3.0059581     | \n",
       "| marital_status |  6.1889564     | \n",
       "| occupation     |  6.8541051     | \n",
       "| relationship   |  4.7061863     | \n",
       "| race           |  3.2044790     | \n",
       "| sex            |  3.8744382     | \n",
       "| capital_gain   | 25.6073567     | \n",
       "| capital_loss   | 12.8919560     | \n",
       "| hours_per_week |  5.0180808     | \n",
       "| native_country | -2.1015123     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Feature        Importance\n",
       "1  age             3.5945641\n",
       "2  workclass       6.2591816\n",
       "3  fnlwgt          0.3867257\n",
       "4  education       3.2916342\n",
       "5  education_num   3.0059581\n",
       "6  marital_status  6.1889564\n",
       "7  occupation      6.8541051\n",
       "8  relationship    4.7061863\n",
       "9  race            3.2044790\n",
       "10 sex             3.8744382\n",
       "11 capital_gain   25.6073567\n",
       "12 capital_loss   12.8919560\n",
       "13 hours_per_week  5.0180808\n",
       "14 native_country -2.1015123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAADwCAMAAADvq0eIAAACylBMVEUAAAAAAIsBAQECAgIF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\nFwAucAHgAtf/A5fST6st7ZTaAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Hint: columns of character data type should be converted into factor before traning and testing the model\n",
    "\n",
    "# read the dataset\n",
    "adult <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', col_names = FALSE)\n",
    "\n",
    "# rename the columns\n",
    "names(adult) <- c('age', 'workclass', 'fnlwgt', 'education','education_num', 'marital_status', \n",
    "                  'occupation', 'relationship', 'race',\n",
    "                  'sex', 'capital_gain', 'capital_loss','hours_per_week', 'native_country', 'income')\n",
    "\n",
    "# convert string columns into facto\n",
    "adult$workclass <- as.factor(adult$workclass)\n",
    "adult$education <- as.factor(adult$education)\n",
    "adult$marital_status <- as.factor(adult$marital_status)\n",
    "adult$occupation <- as.factor(adult$occupation)\n",
    "adult$relationship <- as.factor(adult$relationship)\n",
    "adult$race <- as.factor(adult$race)\n",
    "adult$sex <- as.factor(adult$sex)\n",
    "adult$native_country <- as.factor(adult$native_country)\n",
    "adult$income <- as.factor(adult$income)\n",
    "\n",
    "# train a random forest to predict income\n",
    "adult_rf <- randomForest(income ~ ., data = adult, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(adult_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "[Unsupervised machine learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is the machine learning task of uncovering the hidden structure from \"unlabeled\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quizz:** Which of the following tasks are suitable to solve with clustering?\n",
    "\n",
    "    ☑ Segmenting an MRI image.\n",
    "    ☐ Predicting the category of a product based on its image(s).\n",
    "    ☑ Identifying groups of customers with similar behavior.\n",
    "    ☐ Predicting monthly sales of a store.\n",
    "    ☐ Predicting the type of dog breed from 120 breeds of dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering**\n",
    "    \n",
    "    kmeans_model <- kmeans(x=X, centers=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise, we will use movies data on the [MovieLens 100K Dataset](http://files.grouplens.org/datasets/movielens/ml-100k/u.item) collected from the [MovieLens web site](http://movielens.org). It is available as `data/movies.txt` inside the directory of this lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies <- read_delim('data/movies.txt', delim = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspecting and preprocessing\n",
    "\n",
    "# check top records\n",
    "head(movies)\n",
    "\n",
    "# remove duplicates\n",
    "movies <- distinct(movies)\n",
    "\n",
    "# Mow many movies are tagged as Comedy\n",
    "filter(movies, Comedy == 1) %>% count()\n",
    "\n",
    "# How many movies are tagged as Romance and Drama?\n",
    "filter(movies, Romance == 1 & Drama == 1) %>% count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a k-means cluster\n",
    "k = 5\n",
    "iters = 1000\n",
    "set.seed(1)\n",
    "\n",
    "movies <- select(movies, -Title)\n",
    "movie_kmeans <- kmeans(movies, centers = k, iter.max=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Clustering Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view clustering output\n",
    "str(movie_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster vector, i.e., cluster index of each row\n",
    "movie_kmeans$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid values\n",
    "movie_kmeans$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of clusters, i.e., number of movies in each cluster\n",
    "movie_kmeans$size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squares\n",
    "movie_kmeans$withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining number of clusters**\n",
    "\n",
    "One way to select the number of clusters is by using a **scree plot**. A standard scree plot has the number of clusters on the x-axis, and the sum of the within-cluster sum of squares on the y-axis. The within-cluster sum of squares for a cluster is the sum, across all points in the cluster, of the squared distance between each point and the centroid of the cluster. To determine the best number of clusters using this plot, we want to look for a bend, or elbow, in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(99)\n",
    "\n",
    "# Call kmeans function with centers = 3, centers = 4, etc\n",
    "num_clusters = seq(5, 15,1)\n",
    "\n",
    "# within-cluster sum of squares for all clusters\n",
    "sum_withinss = sapply(num_clusters, function(x) sum(kmeans(movies, centers=x, iter.max=2000)$withinss))\n",
    "\n",
    "# visualize\n",
    "ggplot(mapping = aes(x=num_clusters, y=sum_withinss)) +\n",
    "    geom_line() +\n",
    "    geom_point()\n",
    "    \n",
    "# 12 seems like a good pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Top 10 algorithms in data mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)\n",
    "\n",
    "[R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)\n",
    "\n",
    "[The caret Package](http://topepo.github.io/caret/)\n",
    "\n",
    "[Linear Regression: r-statistics.co](http://r-statistics.co/Linear-Regression.html)\n",
    "\n",
    "[Tutorial: SVM in R](http://math.stanford.edu/~yuany/course/2015.fall/SVM_in_R.pdf)\n",
    "\n",
    "[Artificial Neural Networks in R](https://rpubs.com/julianhatwell/annr)\n",
    "\n",
    "[Cluster analysis in R: determine the optimal number of clusters](https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
